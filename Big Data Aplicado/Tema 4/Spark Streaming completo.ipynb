{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df77c259-b377-4c25-ba3e-7834565788a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introducción a Spark Streaming\n",
    "Spark Streaming permite el procesamiento de flujos de datos continuos e ilimitados en tiempo real. A diferencia del procesamiento batch, trabaja con micro-batches de datos, permitiendo procesarlos de forma distribuida.\n",
    "\n",
    "Esto plantea retos adicionales debido a la impredecibilidad en el ritmo y orden de llegada de los datos. Su uso común incluye cálculos agregados y resúmenes de datos para su consumo por aplicaciones web o motores analíticos. \n",
    "\n",
    "Spark Streaming se ha relacionado históricamente con la capa \"Speed Layer\" de la arquitectura **Lambda** debido a su diseño original, aunque ha evolucionado a un modelo compatible con **Kappa**, con la posibilidad de unificar el procesamiento batch y en streaming en un solo flujo de datos, eliminando la necesidad de dos capas separadas.\n",
    "\n",
    "Ejemplos de uso en la industria:\n",
    "- Monitorización de logs de servidores\n",
    "- Procesamiento de eventos de sensores IoT\n",
    "- Análisis de transacciones financieras en tiempo real\n",
    "- Procesamiento de datos de redes sociales\n",
    "\n",
    "### Streaming en Spark\n",
    "Spark Streaming es una extensión del núcleo de Spark que permite el procesamiento de flujos de datos en vivo ofreciendo tolerancia a fallos, un alto rendimiento y altamente escalable.\n",
    "\n",
    "Los datos se pueden ingestar desde diversas fuentes de datos, como Kafka, sockets TCP, etc.. y se pueden procesar mediante funciones de alto nivel, ya sea mediante el uso de RDD y algoritmos MapReduce, o utilizando DataFrames y la sintaxis SQL. \n",
    "\n",
    "Finalmente, los datos procesados se almacenan en sistemas de ficheros, bases de datos o cuadros de mandos.\n",
    "\n",
    "<img src=\"files/img/03streaming_arch.png\" alt=\"03streaming_arch\">\n",
    "\n",
    "Spark dispone dos soluciones para trabajar con datos en streaming:\n",
    "\n",
    "- Spark DStream: más antigua, conocida como la primera generación, basada en RDDs\n",
    "- Spark Structured Streaming basada en el uso de DataFrames y diseñada para construir aplicaciones que puedan reaccionar a los datos en tiempo real.\n",
    "\n",
    "### DStream\n",
    "Spark DStream (Discretized Stream) es la primera versión y actualmente no se recomienda su uso.\n",
    "\n",
    "Funciona mediante un modelo de micro-batching para dividir los flujos de entrada de datos en fragmentos que son procesados por el núcleo de Spark. Este planteamiento tenía mucho sentido cuando el principal modelo de programación de Spark eran los RDD, ya que cada fragmento recibido se representaba mediante un RDD.\n",
    "\n",
    "Spark DStream recibe los datos de entrada en flujos y los divide en batches, por ejemplo en bloques cada N segundos, los cuales procesa Spark mediante RDD para generar los flujos de resultados procesados:\n",
    "\n",
    "<img src=\"files/img/03streaming_flow.png\" alt=\"03streaming_flow\">\n",
    "\n",
    "### Structured Streaming\n",
    "Spark Structured Streaming es la segunda generación de motor para el tratamiento de datos en streaming, y fue diseñado para ser más rápido, escalable y con mayor tolerancia a los errores que DStream, utilizando el motor de Spark SQL.\n",
    "\n",
    "Además, podemos expresar los procesos en streaming de la misma manera que realizaríamos un proceso batch con datos estáticos. El motor de Spark SQL se encarga de ejecutar los datos de forma continua e incremental, y actualizar el resultado final como datos streaming. Para ello, podemos utilizar el API de Java, Scala, Python o R para expresar las agregaciones, ventanas de eventos, joins de stream a batch, etc.... Finalmente, el sistema asegura la tolerancia de fallos mediante la entrega de cada mensaje una sola vez (**exactly-once**) a través de checkpoints y logs.\n",
    "\n",
    "Los pasos esenciales a realizar al codificar una aplicación en streaming son:\n",
    "\n",
    "1. Especificar uno o más fuentes de datos\n",
    "2. Desarrollar la lógica para manipular los flujos de entrada de datos mediante transformaciones de DataFrames,\n",
    "3. Definir el modo de salida\n",
    "4. Definir el trigger que provoca la lectura\n",
    "5. Indicar el destino de los datos (data sink) donde escribir los resultados.\n",
    "\n",
    "<img src=\"files/img/03streaming_fases.jpg\" alt=\"03streaming_fases\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1182857f-0288-4516-80ed-cea560e53976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Configuración del Entorno\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#### Crear una SparkSession con soporte para Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Streaming\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd5fc41-41cd-49a5-a921-613c0c87b7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Elementos\n",
    "La idea básica al trabajar los datos en streaming es similar a tener una tabla de entrada de tamaño ilimitado, y conforme llegan nuevos datos, tratarlos como un nuevo conjunto de filas que se adjuntan a la tabla.\n",
    "\n",
    "<img src=\"files/img/03streaming_datatable.png\" alt=\"03streaming_datatable\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbf52ff-c44f-463f-9ac9-8e9957fc1881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fuentes de Datos en Structured Streaming\n",
    "A diferencia del procesamiento batch, donde los datos provienen de fuentes estáticas como archivos en HDFS o S3, en Structured Streaming las fuentes generan datos de forma continua.\n",
    "\n",
    "Fuentes de datos disponibles:\n",
    "- Ficheros: Lee archivos en streaming desde un directorio (CSV, JSON, Parquet, etc.).\n",
    "- Kafka: Consume datos en tiempo real desde un clúster Kafka.\n",
    "- Socket: Recibe texto en streaming desde una conexión de socket (solo para pruebas).\n",
    "- Rate: Genera datos sintéticos con timestamps y valores secuenciales (útil para pruebas y benchmarking).\n",
    "- Tabla (desde Spark 3.1): Lee datos en tiempo real desde una tabla de Spark SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c70058-dbe0-4c92-9012-f05b405ce1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>value</th></tr></thead><tbody><tr><td>2025-03-17T18:31:47.541Z</td><td>0</td></tr><tr><td>2025-03-17T18:31:48.541Z</td><td>1</td></tr><tr><td>2025-03-17T18:31:49.541Z</td><td>2</td></tr><tr><td>2025-03-17T18:31:50.541Z</td><td>3</td></tr><tr><td>2025-03-17T18:31:51.541Z</td><td>4</td></tr><tr><td>2025-03-17T18:31:52.541Z</td><td>5</td></tr><tr><td>2025-03-17T18:31:53.541Z</td><td>6</td></tr><tr><td>2025-03-17T18:31:54.541Z</td><td>7</td></tr><tr><td>2025-03-17T18:31:55.541Z</td><td>8</td></tr><tr><td>2025-03-17T18:31:56.541Z</td><td>9</td></tr><tr><td>2025-03-17T18:31:57.541Z</td><td>10</td></tr><tr><td>2025-03-17T18:31:58.541Z</td><td>11</td></tr><tr><td>2025-03-17T18:31:59.541Z</td><td>12</td></tr><tr><td>2025-03-17T18:32:00.541Z</td><td>13</td></tr><tr><td>2025-03-17T18:32:01.541Z</td><td>14</td></tr><tr><td>2025-03-17T18:32:02.541Z</td><td>15</td></tr><tr><td>2025-03-17T18:32:03.541Z</td><td>16</td></tr><tr><td>2025-03-17T18:32:04.541Z</td><td>17</td></tr><tr><td>2025-03-17T18:32:05.541Z</td><td>18</td></tr><tr><td>2025-03-17T18:32:06.541Z</td><td>19</td></tr><tr><td>2025-03-17T18:32:07.541Z</td><td>20</td></tr><tr><td>2025-03-17T18:32:08.541Z</td><td>21</td></tr><tr><td>2025-03-17T18:32:09.541Z</td><td>22</td></tr><tr><td>2025-03-17T18:32:10.541Z</td><td>23</td></tr><tr><td>2025-03-17T18:32:11.541Z</td><td>24</td></tr><tr><td>2025-03-17T18:32:12.541Z</td><td>25</td></tr><tr><td>2025-03-17T18:32:13.541Z</td><td>26</td></tr><tr><td>2025-03-17T18:32:14.541Z</td><td>27</td></tr><tr><td>2025-03-17T18:32:15.541Z</td><td>28</td></tr><tr><td>2025-03-17T18:32:16.541Z</td><td>29</td></tr><tr><td>2025-03-17T18:32:17.541Z</td><td>30</td></tr><tr><td>2025-03-17T18:32:18.541Z</td><td>31</td></tr><tr><td>2025-03-17T18:32:19.541Z</td><td>32</td></tr><tr><td>2025-03-17T18:32:20.541Z</td><td>33</td></tr><tr><td>2025-03-17T18:32:21.541Z</td><td>34</td></tr><tr><td>2025-03-17T18:32:22.541Z</td><td>35</td></tr><tr><td>2025-03-17T18:32:23.541Z</td><td>36</td></tr><tr><td>2025-03-17T18:32:24.541Z</td><td>37</td></tr><tr><td>2025-03-17T18:32:25.541Z</td><td>38</td></tr><tr><td>2025-03-17T18:32:26.541Z</td><td>39</td></tr><tr><td>2025-03-17T18:32:27.541Z</td><td>40</td></tr><tr><td>2025-03-17T18:32:28.541Z</td><td>41</td></tr><tr><td>2025-03-17T18:32:29.541Z</td><td>42</td></tr><tr><td>2025-03-17T18:32:30.541Z</td><td>43</td></tr><tr><td>2025-03-17T18:32:31.541Z</td><td>44</td></tr><tr><td>2025-03-17T18:32:32.541Z</td><td>45</td></tr><tr><td>2025-03-17T18:32:33.541Z</td><td>46</td></tr><tr><td>2025-03-17T18:32:34.541Z</td><td>47</td></tr><tr><td>2025-03-17T18:32:35.541Z</td><td>48</td></tr><tr><td>2025-03-17T18:32:36.541Z</td><td>49</td></tr><tr><td>2025-03-17T18:32:37.541Z</td><td>50</td></tr><tr><td>2025-03-17T18:32:38.541Z</td><td>51</td></tr><tr><td>2025-03-17T18:32:39.541Z</td><td>52</td></tr><tr><td>2025-03-17T18:32:40.541Z</td><td>53</td></tr><tr><td>2025-03-17T18:32:41.541Z</td><td>54</td></tr><tr><td>2025-03-17T18:32:42.541Z</td><td>55</td></tr><tr><td>2025-03-17T18:32:43.541Z</td><td>56</td></tr><tr><td>2025-03-17T18:32:44.541Z</td><td>57</td></tr><tr><td>2025-03-17T18:32:45.541Z</td><td>58</td></tr><tr><td>2025-03-17T18:32:46.541Z</td><td>59</td></tr><tr><td>2025-03-17T18:32:47.541Z</td><td>60</td></tr><tr><td>2025-03-17T18:32:48.541Z</td><td>61</td></tr><tr><td>2025-03-17T18:32:49.541Z</td><td>62</td></tr><tr><td>2025-03-17T18:32:50.541Z</td><td>63</td></tr><tr><td>2025-03-17T18:32:51.541Z</td><td>64</td></tr><tr><td>2025-03-17T18:32:52.541Z</td><td>65</td></tr><tr><td>2025-03-17T18:32:53.541Z</td><td>66</td></tr><tr><td>2025-03-17T18:32:54.541Z</td><td>67</td></tr><tr><td>2025-03-17T18:32:55.541Z</td><td>68</td></tr><tr><td>2025-03-17T18:32:56.541Z</td><td>69</td></tr><tr><td>2025-03-17T18:32:57.541Z</td><td>70</td></tr><tr><td>2025-03-17T18:32:58.541Z</td><td>71</td></tr><tr><td>2025-03-17T18:32:59.541Z</td><td>72</td></tr><tr><td>2025-03-17T18:33:00.541Z</td><td>73</td></tr><tr><td>2025-03-17T18:33:01.541Z</td><td>74</td></tr><tr><td>2025-03-17T18:33:02.541Z</td><td>75</td></tr><tr><td>2025-03-17T18:33:03.541Z</td><td>76</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-03-17T18:31:47.541Z",
         0
        ],
        [
         "2025-03-17T18:31:48.541Z",
         1
        ],
        [
         "2025-03-17T18:31:49.541Z",
         2
        ],
        [
         "2025-03-17T18:31:50.541Z",
         3
        ],
        [
         "2025-03-17T18:31:51.541Z",
         4
        ],
        [
         "2025-03-17T18:31:52.541Z",
         5
        ],
        [
         "2025-03-17T18:31:53.541Z",
         6
        ],
        [
         "2025-03-17T18:31:54.541Z",
         7
        ],
        [
         "2025-03-17T18:31:55.541Z",
         8
        ],
        [
         "2025-03-17T18:31:56.541Z",
         9
        ],
        [
         "2025-03-17T18:31:57.541Z",
         10
        ],
        [
         "2025-03-17T18:31:58.541Z",
         11
        ],
        [
         "2025-03-17T18:31:59.541Z",
         12
        ],
        [
         "2025-03-17T18:32:00.541Z",
         13
        ],
        [
         "2025-03-17T18:32:01.541Z",
         14
        ],
        [
         "2025-03-17T18:32:02.541Z",
         15
        ],
        [
         "2025-03-17T18:32:03.541Z",
         16
        ],
        [
         "2025-03-17T18:32:04.541Z",
         17
        ],
        [
         "2025-03-17T18:32:05.541Z",
         18
        ],
        [
         "2025-03-17T18:32:06.541Z",
         19
        ],
        [
         "2025-03-17T18:32:07.541Z",
         20
        ],
        [
         "2025-03-17T18:32:08.541Z",
         21
        ],
        [
         "2025-03-17T18:32:09.541Z",
         22
        ],
        [
         "2025-03-17T18:32:10.541Z",
         23
        ],
        [
         "2025-03-17T18:32:11.541Z",
         24
        ],
        [
         "2025-03-17T18:32:12.541Z",
         25
        ],
        [
         "2025-03-17T18:32:13.541Z",
         26
        ],
        [
         "2025-03-17T18:32:14.541Z",
         27
        ],
        [
         "2025-03-17T18:32:15.541Z",
         28
        ],
        [
         "2025-03-17T18:32:16.541Z",
         29
        ],
        [
         "2025-03-17T18:32:17.541Z",
         30
        ],
        [
         "2025-03-17T18:32:18.541Z",
         31
        ],
        [
         "2025-03-17T18:32:19.541Z",
         32
        ],
        [
         "2025-03-17T18:32:20.541Z",
         33
        ],
        [
         "2025-03-17T18:32:21.541Z",
         34
        ],
        [
         "2025-03-17T18:32:22.541Z",
         35
        ],
        [
         "2025-03-17T18:32:23.541Z",
         36
        ],
        [
         "2025-03-17T18:32:24.541Z",
         37
        ],
        [
         "2025-03-17T18:32:25.541Z",
         38
        ],
        [
         "2025-03-17T18:32:26.541Z",
         39
        ],
        [
         "2025-03-17T18:32:27.541Z",
         40
        ],
        [
         "2025-03-17T18:32:28.541Z",
         41
        ],
        [
         "2025-03-17T18:32:29.541Z",
         42
        ],
        [
         "2025-03-17T18:32:30.541Z",
         43
        ],
        [
         "2025-03-17T18:32:31.541Z",
         44
        ],
        [
         "2025-03-17T18:32:32.541Z",
         45
        ],
        [
         "2025-03-17T18:32:33.541Z",
         46
        ],
        [
         "2025-03-17T18:32:34.541Z",
         47
        ],
        [
         "2025-03-17T18:32:35.541Z",
         48
        ],
        [
         "2025-03-17T18:32:36.541Z",
         49
        ],
        [
         "2025-03-17T18:32:37.541Z",
         50
        ],
        [
         "2025-03-17T18:32:38.541Z",
         51
        ],
        [
         "2025-03-17T18:32:39.541Z",
         52
        ],
        [
         "2025-03-17T18:32:40.541Z",
         53
        ],
        [
         "2025-03-17T18:32:41.541Z",
         54
        ],
        [
         "2025-03-17T18:32:42.541Z",
         55
        ],
        [
         "2025-03-17T18:32:43.541Z",
         56
        ],
        [
         "2025-03-17T18:32:44.541Z",
         57
        ],
        [
         "2025-03-17T18:32:45.541Z",
         58
        ],
        [
         "2025-03-17T18:32:46.541Z",
         59
        ],
        [
         "2025-03-17T18:32:47.541Z",
         60
        ],
        [
         "2025-03-17T18:32:48.541Z",
         61
        ],
        [
         "2025-03-17T18:32:49.541Z",
         62
        ],
        [
         "2025-03-17T18:32:50.541Z",
         63
        ],
        [
         "2025-03-17T18:32:51.541Z",
         64
        ],
        [
         "2025-03-17T18:32:52.541Z",
         65
        ],
        [
         "2025-03-17T18:32:53.541Z",
         66
        ],
        [
         "2025-03-17T18:32:54.541Z",
         67
        ],
        [
         "2025-03-17T18:32:55.541Z",
         68
        ],
        [
         "2025-03-17T18:32:56.541Z",
         69
        ],
        [
         "2025-03-17T18:32:57.541Z",
         70
        ],
        [
         "2025-03-17T18:32:58.541Z",
         71
        ],
        [
         "2025-03-17T18:32:59.541Z",
         72
        ],
        [
         "2025-03-17T18:33:00.541Z",
         73
        ],
        [
         "2025-03-17T18:33:01.541Z",
         74
        ],
        [
         "2025-03-17T18:33:02.541Z",
         75
        ],
        [
         "2025-03-17T18:33:03.541Z",
         76
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulación de datos en streaming\n",
    "lineasDF = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "display(lineasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0935772-84cd-4c17-8976-0b870c7cb661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>resto</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>12</td></tr><tr><td>7</td><td>12</td></tr><tr><td>6</td><td>12</td></tr><tr><td>9</td><td>11</td></tr><tr><td>5</td><td>12</td></tr><tr><td>1</td><td>12</td></tr><tr><td>3</td><td>12</td></tr><tr><td>8</td><td>12</td></tr><tr><td>2</td><td>12</td></tr><tr><td>4</td><td>12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         12
        ],
        [
         7,
         12
        ],
        [
         6,
         12
        ],
        [
         9,
         11
        ],
        [
         5,
         12
        ],
        [
         1,
         12
        ],
        [
         3,
         12
        ],
        [
         8,
         12
        ],
        [
         2,
         12
        ],
        [
         4,
         12
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "resto",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Simulación de datos en streaming\n",
    "lineasDF = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# Mostramos resultado agrupado\n",
    "restoDF = lineasDF.withColumn(\"resto\",F.col(\"value\") % 10)\n",
    "cantidadDF = restoDF.groupBy(\"resto\").count()\n",
    "\n",
    "display(cantidadDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cababe-b9c3-4f7d-93a4-37fd622f7c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>CitySales ($)</th></tr></thead><tbody><tr><td>San Francisco</td><td>$4,693,982.54</td></tr><tr><td>Los Angeles</td><td>$3,112,507.07</td></tr><tr><td>New York City</td><td>$2,588,313.60</td></tr><tr><td>Boston</td><td>$2,006,944.56</td></tr><tr><td>Atlanta</td><td>$1,585,109.51</td></tr><tr><td>Dallas</td><td>$1,512,350.36</td></tr><tr><td>Seattle</td><td>$1,508,763.13</td></tr><tr><td>Portland</td><td>$1,333,897.87</td></tr><tr><td>Austin</td><td>$1,001,007.34</td></tr><tr><td>null</td><td>$nu</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "San Francisco",
         "$4,693,982.54"
        ],
        [
         "Los Angeles",
         "$3,112,507.07"
        ],
        [
         "New York City",
         "$2,588,313.60"
        ],
        [
         "Boston",
         "$2,006,944.56"
        ],
        [
         "Atlanta",
         "$1,585,109.51"
        ],
        [
         "Dallas",
         "$1,512,350.36"
        ],
        [
         "Seattle",
         "$1,508,763.13"
        ],
        [
         "Portland",
         "$1,333,897.87"
        ],
        [
         "Austin",
         "$1,001,007.34"
        ],
        [
         null,
         "$nu"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CitySales ($)",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lee los ficheros que se van añadiendo a un directorio\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Esquema ventas\n",
    "esquemaVentas = StructType([\n",
    "    StructField(\"Order ID\", IntegerType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity Ordered\", IntegerType(), True),\n",
    "    StructField(\"Price Each\", DoubleType(), True),\n",
    "    StructField(\"Order Date\", StringType(), True),\n",
    "    StructField(\"Purchase Address\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leemos ficheros csv del directorio salesstreaming\n",
    "dfventas = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(esquemaVentas) \\\n",
    "    .csv(\"dbfs:/FileStore/salesstreaming\")\n",
    "\n",
    "# Crear campos adicionales en el dataframe Ventas: year, month, state, city, CP a partir de los campos Order Date y Purchase Address\n",
    "dfventas = dfventas.withColumn(\"timestamp\", F.to_timestamp(\"Order Date\",\"MM/dd/yy HH:mm\"))\n",
    "dfventas = dfventas.withColumn(\"year\", F.year(F.col(\"timestamp\"))).withColumn(\"month\", F.month(F.col(\"timestamp\")))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"address\", F.split(F.col(\"Purchase Address\"), \",\"))\n",
    "dfventas = dfventas.withColumn(\"state\", F.substring(F.col(\"address\")[2],1,3))\n",
    "dfventas = dfventas.withColumn(\"CP\", F.substring(F.col(\"address\")[2],4,9))\n",
    "dfventas = dfventas.withColumn(\"city\", F.trim(F.col(\"address\")[1]))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"Sales\", F.col(\"Quantity Ordered\")*F.col(\"Price Each\"))\n",
    "\n",
    "# Ciudades con mayores ingresos\n",
    "dfventasCiudad = dfventas.groupBy(F.col(\"city\")).agg(F.sum(F.col(\"Sales\")).alias(\"CitySales\")).orderBy(F.desc(F.col(\"CitySales\"))) \\\n",
    "    .withColumn(\"CitySales ($)\", F.format_string(\"$%,.2f\", F.col(\"CitySales\")))\n",
    "\n",
    "display(dfventasCiudad.select(\"city\", \"CitySales ($)\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d61a8d-ec97-49a2-8189-662fd2bbc51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear campos adicionales en el dataframe Ventas: year, month, state, city, CP a partir de los campos Order Date y Purchase Address\n",
    "dfventas = dfventas.withColumn(\"timestamp\", F.to_timestamp(\"Order Date\",\"MM/dd/yy HH:mm\"))\n",
    "dfventas = dfventas.withColumn(\"year\", F.year(F.col(\"timestamp\"))).withColumn(\"month\", F.month(F.col(\"timestamp\")))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"address\", F.split(F.col(\"Purchase Address\"), \",\"))\n",
    "dfventas = dfventas.withColumn(\"state\", F.substring(F.col(\"address\")[2],1,3))\n",
    "dfventas = dfventas.withColumn(\"CP\", F.substring(F.col(\"address\")[2],4,9))\n",
    "dfventas = dfventas.withColumn(\"city\", F.trim(F.col(\"address\")[1]))\n",
    "\n",
    "# Obtener el día con mayores ingresos\n",
    "dfventas = dfventas.withColumn(\"Sales\", F.col(\"Quantity Ordered\")*F.col(\"Price Each\"))\n",
    "dfventas.groupBy(F.to_date(F.col(\"timestamp\"))).agg(F.sum(F.col(\"Sales\")).alias(\"DailySales\")) \\\n",
    "    .orderBy(F.col(\"DailySales\"),ascending=False).show(3)\n",
    "\n",
    "# Obtener el producto más vendido (por cantidad total) y qué ingresos ha generado en total.\n",
    "dfventas.groupBy(F.col(\"Product\")) \\\n",
    "    .agg( \\\n",
    "        F.sum(F.col(\"Quantity Ordered\")).alias(\"TotalQty\"), \\\n",
    "        F.sum(F.col(\"Sales\")) \\\n",
    "    ).orderBy(F.col(\"TotalQty\"),ascending=False).show(3)\n",
    "\n",
    "# Listar las 10 ciudades con mayores ventas (en ingresos).\n",
    "dfventas.groupBy(F.col(\"city\")).agg(F.sum(F.col(\"Sales\")).alias(\"CitySales\")).orderBy(F.desc(F.col(\"CitySales\"))).show(10)\n",
    "\n",
    "# Tabla de número de pedidos e importe por horas (campo hour extraído de Order Date).\n",
    "dfventas.groupBy(F.hour(F.col(\"timestamp\")).alias(\"Hour\")) \\\n",
    "    .agg(F.sum(F.col(\"Quantity Ordered\")).alias(\"TotalQty\"),F.sum(F.col(\"Sales\"))) \\\n",
    "    .orderBy(F.col(\"Hour\")).show(24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfdcd4ec-5476-46cc-91f4-c3c1cb3a2b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opciones clave por fuente\n",
    "\n",
    "# Archivos\n",
    "\n",
    "df = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"path\", \"/ruta/al/directorio\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\  # Nº de archivos leídos por trigger\n",
    "    .option(\"latestFirst\", True) \\      # Leer los archivos más recientes primero\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba6869b-5dfc-4d3c-93c6-ef9d2185a362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Socket: sólo pruebas, sin tolerancia a fallos\n",
    "\n",
    "df = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ca9dbb-11e2-4270-9cb3-bcb6cf5221f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kafka\n",
    "\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\  # \"earliest\" para leer desde el inicio\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b9e94d-ea63-401a-83bf-50bb567f03aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rate: genera datos automáticamente, con un timestamp y value creciente\n",
    "\n",
    "df = spark.readStream.format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d5c488-92b9-46aa-bb40-9263b09bcb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tablas: Útil para persistencia y consultas en tiempo real.\n",
    "\n",
    "df = spark.readStream.table(\"nombre_tabla\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6085913a-517f-4463-b3ee-acb132b65c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modos de salida (outputMode)\n",
    "\n",
    "Cuando escribimos un DataFrame en streaming (writeStream), debemos especificar cómo se actualizan los datos en el destino:\n",
    "\n",
    "| Modo      | Descripción                                                                 |\n",
    "|-----------|-----------------------------------------------------------------------------|\n",
    "| append    | Solo agrega nuevas filas al destino. No admite actualizaciones o eliminaciones. |\n",
    "| complete  | Escribe todos los resultados en cada trigger. Útil para agregaciones.       |\n",
    "| update    | Similar a append, pero actualiza filas ya procesadas. Necesario cuando se usan operaciones de estado (como agregaciones incrementales). |\n",
    "\n",
    "## Triggers (trigger)\n",
    "\n",
    "Define cada cuánto tiempo se procesan los datos:\n",
    "\n",
    "| Trigger                              | Descripción                                                      |\n",
    "|--------------------------------------|------------------------------------------------------------------|\n",
    "| trigger(processingTime=\"10 seconds\") | Ejecuta la consulta cada 10 segundos.                           |\n",
    "| trigger(once=True)                  | Ejecuta la consulta solo una vez, procesando todos los datos disponibles. |\n",
    "| trigger(continuous=\"1 second\")      | Para baja latencia (solo con fuentes compatibles como Kafka).   |\n",
    "\n",
    "## Destinos (sinks)\n",
    "\n",
    "El sink define dónde se escriben los datos procesados:\n",
    "\n",
    "| Sink       | Descripción                                                        |\n",
    "|------------|--------------------------------------------------------------------|\n",
    "| console    | Muestra los resultados en la consola (útil para pruebas).          |\n",
    "| memory     | Guarda en memoria (puede ser consultado con SQL).                  |\n",
    "| file       | Escribe en formato JSON, Parquet, CSV, etc.                       |\n",
    "| kafka      | Escribe mensajes en un topic de Apache Kafka.                     |\n",
    "| foreach    | Permite escribir en un destino personalizado (API de Python/Java). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9fbe8d-c380-4c02-a404-cf1e234151fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tolerancia a fallos y Checkpoints\n",
    "\n",
    "Spark Streaming garantiza tolerancia a fallos con Checkpoints y WAL (Write-Ahead Logs).\n",
    "\n",
    "**Checkpointing:** Almacena el estado del streaming en un directorio para recuperarlo en caso de fallo.\n",
    "\n",
    "**Habilitación de checkpoints:** Esto permite recuperación automática en caso de fallo del nodo o reinicio de la aplicación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7a04e8-9e1c-4262-864f-c815fb5d66f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/ruta/salida\") \\\n",
    "    .option(\"checkpointLocation\", \"/ruta/checkpoints\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "428e240a-7342-4dc1-9877-3d623aef3d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Windowing (Ventanas temporales)\n",
    "\n",
    "Permite agrupar datos en intervalos de tiempo en lugar de procesar cada evento individualmente.\n",
    "\n",
    "**Ejemplo:** Contar eventos en ventanas de 10 segundos con 5 segundos de solapamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac02ca8-8da4-43c4-a224-f54fb68e4987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "df_window = df.groupBy(window(\"timestamp\", \"10 seconds\", \"5 seconds\")).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6a8bece-f73b-4a19-8e37-31fca1d2fba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Watermarking\n",
    "\n",
    "Controla la cantidad de datos antiguos que se retienen en memoria, permitiendo manejar retrasos en la llegada de datos sin consumir demasiados recursos.\n",
    "\n",
    "**Ejemplo:** Mantener datos de hasta 10 minutos en un `groupBy` basado en ventana de tiempo:\n",
    "\n",
    "Evita acumulación de estado indefinida y mejora la escalabilidad del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def540c3-1511-4ee5-b7c0-15b318bca140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_watermarked = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ffef31-1076-4080-a72a-3765e405d5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Joins en Streaming\n",
    "Se pueden realizar uniones entre:\n",
    "\n",
    "- Streaming vs Batch\n",
    "- Streaming vs Streaming (con restricciones)\n",
    "\n",
    "Uso común en correlación de eventos, como unir registros de compras en streaming con datos de clientes en batch.\n",
    "\n",
    "Ejemplo de join entre un flujo de datos y una tabla estática:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff08535-e752-4b61-aef6-40319db99ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_streaming.join(df_estatico, \"id\", \"inner\")\n",
    "\n",
    "df1.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .join(df2.withWatermark(\"timestamp\", \"10 minutes\"), \"id\", \"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad0eab0-307f-454f-b4c0-fbcb5efff2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>21932</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         21932
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select count(*) from samples.nyctaxi.trips"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 885119335613128,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
