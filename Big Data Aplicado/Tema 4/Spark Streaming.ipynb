{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df77c259-b377-4c25-ba3e-7834565788a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introducci칩n a Spark Streaming\n",
    "Spark Streaming permite el procesamiento de flujos de datos continuos e ilimitados en tiempo real. A diferencia del procesamiento batch, trabaja con micro-batches de datos, permitiendo procesarlos de forma distribuida.\n",
    "\n",
    "Esto plantea retos adicionales debido a la impredecibilidad en el ritmo y orden de llegada de los datos. Su uso com칰n incluye c치lculos agregados y res칰menes de datos para su consumo por aplicaciones web o motores anal칤ticos. \n",
    "\n",
    "Spark Streaming se ha relacionado hist칩ricamente con la capa \"Speed Layer\" de la arquitectura **Lambda** debido a su dise침o original, aunque ha evolucionado a un modelo compatible con **Kappa**, con la posibilidad de unificar el procesamiento batch y en streaming en un solo flujo de datos, eliminando la necesidad de dos capas separadas.\n",
    "\n",
    "Ejemplos de uso en la industria:\n",
    "- Monitorizaci칩n de logs de servidores\n",
    "- Procesamiento de eventos de sensores IoT\n",
    "- An치lisis de transacciones financieras en tiempo real\n",
    "- Procesamiento de datos de redes sociales\n",
    "\n",
    "### Streaming en Spark\n",
    "Spark Streaming es una extensi칩n del n칰cleo de Spark que permite el procesamiento de flujos de datos en vivo ofreciendo tolerancia a fallos, un alto rendimiento y altamente escalable.\n",
    "\n",
    "Los datos se pueden ingestar desde diversas fuentes de datos, como Kafka, sockets TCP, etc.. y se pueden procesar mediante funciones de alto nivel, ya sea mediante el uso de RDD y algoritmos MapReduce, o utilizando DataFrames y la sintaxis SQL. \n",
    "\n",
    "Finalmente, los datos procesados se almacenan en sistemas de ficheros, bases de datos o cuadros de mandos.\n",
    "\n",
    "<img src=\"files/img/03streaming_arch.png\" alt=\"03streaming_arch\">\n",
    "\n",
    "Spark dispone dos soluciones para trabajar con datos en streaming:\n",
    "\n",
    "- Spark DStream: m치s antigua, conocida como la primera generaci칩n, basada en RDDs\n",
    "- Spark Structured Streaming basada en el uso de DataFrames y dise침ada para construir aplicaciones que puedan reaccionar a los datos en tiempo real.\n",
    "\n",
    "### DStream\n",
    "Spark DStream (Discretized Stream) es la primera versi칩n y actualmente no se recomienda su uso.\n",
    "\n",
    "Funciona mediante un modelo de micro-batching para dividir los flujos de entrada de datos en fragmentos que son procesados por el n칰cleo de Spark. Este planteamiento ten칤a mucho sentido cuando el principal modelo de programaci칩n de Spark eran los RDD, ya que cada fragmento recibido se representaba mediante un RDD.\n",
    "\n",
    "Spark DStream recibe los datos de entrada en flujos y los divide en batches, por ejemplo en bloques cada N segundos, los cuales procesa Spark mediante RDD para generar los flujos de resultados procesados:\n",
    "\n",
    "<img src=\"files/img/03streaming_flow.png\" alt=\"03streaming_flow\">\n",
    "\n",
    "### Structured Streaming\n",
    "Spark Structured Streaming es la segunda generaci칩n de motor para el tratamiento de datos en streaming, y fue dise침ado para ser m치s r치pido, escalable y con mayor tolerancia a los errores que DStream, utilizando el motor de Spark SQL.\n",
    "\n",
    "Adem치s, podemos expresar los procesos en streaming de la misma manera que realizar칤amos un proceso batch con datos est치ticos. El motor de Spark SQL se encarga de ejecutar los datos de forma continua e incremental, y actualizar el resultado final como datos streaming. Para ello, podemos utilizar el API de Java, Scala, Python o R para expresar las agregaciones, ventanas de eventos, joins de stream a batch, etc.... Finalmente, el sistema asegura la tolerancia de fallos mediante la entrega de cada mensaje una sola vez (**exactly-once**) a trav칠s de checkpoints y logs.\n",
    "\n",
    "Los pasos esenciales a realizar al codificar una aplicaci칩n en streaming son:\n",
    "\n",
    "1. Especificar uno o m치s fuentes de datos\n",
    "2. Desarrollar la l칩gica para manipular los flujos de entrada de datos mediante transformaciones de DataFrames,\n",
    "3. Definir el modo de salida\n",
    "4. Definir el trigger que provoca la lectura\n",
    "5. Indicar el destino de los datos (data sink) donde escribir los resultados.\n",
    "\n",
    "<img src=\"files/img/03streaming_fases.jpg\" alt=\"03streaming_fases\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1182857f-0288-4516-80ed-cea560e53976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Configuraci칩n del Entorno\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#### Crear una SparkSession con soporte para Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Streaming\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd5fc41-41cd-49a5-a921-613c0c87b7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Elementos\n",
    "La idea b치sica al trabajar los datos en streaming es similar a tener una tabla de entrada de tama침o ilimitado, y conforme llegan nuevos datos, tratarlos como un nuevo conjunto de filas que se adjuntan a la tabla.\n",
    "\n",
    "<img src=\"files/img/03streaming_datatable.png\" alt=\"03streaming_datatable\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbf52ff-c44f-463f-9ac9-8e9957fc1881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fuentes de Datos en Structured Streaming\n",
    "A diferencia del procesamiento batch, donde los datos provienen de fuentes est치ticas como archivos en HDFS o S3, en Structured Streaming las fuentes generan datos de forma continua.\n",
    "\n",
    "Fuentes de datos disponibles:\n",
    "- Ficheros: Lee archivos en streaming desde un directorio (CSV, JSON, Parquet, etc.).\n",
    "- Kafka: Consume datos en tiempo real desde un cl칰ster Kafka.\n",
    "- Socket: Recibe texto en streaming desde una conexi칩n de socket (solo para pruebas).\n",
    "- Rate: Genera datos sint칠ticos con timestamps y valores secuenciales (칰til para pruebas y benchmarking).\n",
    "- Tabla (desde Spark 3.1): Lee datos en tiempo real desde una tabla de Spark SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c70058-dbe0-4c92-9012-f05b405ce1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulaci칩n de datos en streaming\n",
    "lineasDF = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "display(lineasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0935772-84cd-4c17-8976-0b870c7cb661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulaci칩n de datos en streaming\n",
    "lineasDF = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# Mostramos resultado agrupado\n",
    "restoDF = lineasDF.withColumn(\"resto\",col(\"value\") % 10)\n",
    "cantidadDF = restoDF.groupBy(\"resto\").count()\n",
    "\n",
    "display(cantidadDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cababe-b9c3-4f7d-93a4-37fd622f7c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>CitySales ($)</th></tr></thead><tbody><tr><td>San Francisco</td><td>$2,490,980.51</td></tr><tr><td>Los Angeles</td><td>$1,611,156.95</td></tr><tr><td>New York City</td><td>$1,381,432.87</td></tr><tr><td>Boston</td><td>$1,070,516.65</td></tr><tr><td>Atlanta</td><td>$841,996.12</td></tr><tr><td>Seattle</td><td>$807,395.08</td></tr><tr><td>Dallas</td><td>$804,232.70</td></tr><tr><td>Portland</td><td>$692,620.55</td></tr><tr><td>Austin</td><td>$523,607.34</td></tr><tr><td>null</td><td>$nu</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "San Francisco",
         "$2,490,980.51"
        ],
        [
         "Los Angeles",
         "$1,611,156.95"
        ],
        [
         "New York City",
         "$1,381,432.87"
        ],
        [
         "Boston",
         "$1,070,516.65"
        ],
        [
         "Atlanta",
         "$841,996.12"
        ],
        [
         "Seattle",
         "$807,395.08"
        ],
        [
         "Dallas",
         "$804,232.70"
        ],
        [
         "Portland",
         "$692,620.55"
        ],
        [
         "Austin",
         "$523,607.34"
        ],
        [
         null,
         "$nu"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CitySales ($)",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lee los ficheros que se van a침adiendo a un directorio\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Esquema ventas\n",
    "esquemaVentas = StructType([\n",
    "    StructField(\"Order ID\", IntegerType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity Ordered\", IntegerType(), True),\n",
    "    StructField(\"Price Each\", DoubleType(), True),\n",
    "    StructField(\"Order Date\", StringType(), True),\n",
    "    StructField(\"Purchase Address\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leemos ficheros csv del directorio salesstreaming\n",
    "dfventas = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(esquemaVentas) \\\n",
    "    .csv(\"dbfs:/FileStore/salesstreaming\")\n",
    "\n",
    "# Crear campos adicionales en el dataframe Ventas: year, month, state, city, CP a partir de los campos Order Date y Purchase Address\n",
    "dfventas = dfventas.withColumn(\"timestamp\", F.to_timestamp(\"Order Date\",\"MM/dd/yy HH:mm\"))\n",
    "dfventas = dfventas.withColumn(\"year\", F.year(F.col(\"timestamp\"))).withColumn(\"month\", F.month(F.col(\"timestamp\")))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"address\", F.split(F.col(\"Purchase Address\"), \",\"))\n",
    "dfventas = dfventas.withColumn(\"state\", F.substring(F.col(\"address\")[2],1,3))\n",
    "dfventas = dfventas.withColumn(\"CP\", F.substring(F.col(\"address\")[2],4,9))\n",
    "dfventas = dfventas.withColumn(\"city\", F.trim(F.col(\"address\")[1]))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"Sales\", F.col(\"Quantity Ordered\")*F.col(\"Price Each\"))\n",
    "\n",
    "# Ciudades con mayores ingresos\n",
    "dfventasCiudad = dfventas.groupBy(F.col(\"city\")).agg(F.sum(F.col(\"Sales\")).alias(\"CitySales\")).orderBy(F.desc(F.col(\"CitySales\"))) \\\n",
    "    .withColumn(\"CitySales ($)\", format_string(\"$%,.2f\", col(\"CitySales\")))\n",
    "\n",
    "display(dfventasCiudad.select(\"city\", \"CitySales ($)\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d61a8d-ec97-49a2-8189-662fd2bbc51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear campos adicionales en el dataframe Ventas: year, month, state, city, CP a partir de los campos Order Date y Purchase Address\n",
    "dfventas = dfventas.withColumn(\"timestamp\", F.to_timestamp(\"Order Date\",\"MM/dd/yy HH:mm\"))\n",
    "dfventas = dfventas.withColumn(\"year\", F.year(F.col(\"timestamp\"))).withColumn(\"month\", F.month(F.col(\"timestamp\")))\n",
    "\n",
    "dfventas = dfventas.withColumn(\"address\", F.split(F.col(\"Purchase Address\"), \",\"))\n",
    "dfventas = dfventas.withColumn(\"state\", F.substring(F.col(\"address\")[2],1,3))\n",
    "dfventas = dfventas.withColumn(\"CP\", F.substring(F.col(\"address\")[2],4,9))\n",
    "dfventas = dfventas.withColumn(\"city\", F.trim(F.col(\"address\")[1]))\n",
    "\n",
    "# Obtener el d칤a con mayores ingresos\n",
    "dfventas = dfventas.withColumn(\"Sales\", F.col(\"Quantity Ordered\")*F.col(\"Price Each\"))\n",
    "dfventas.groupBy(F.to_date(F.col(\"timestamp\"))).agg(F.sum(F.col(\"Sales\")).alias(\"DailySales\")) \\\n",
    "    .orderBy(F.col(\"DailySales\"),ascending=False).show(3)\n",
    "\n",
    "# Obtener el producto m치s vendido (por cantidad total) y qu칠 ingresos ha generado en total.\n",
    "dfventas.groupBy(F.col(\"Product\")) \\\n",
    "    .agg( \\\n",
    "        F.sum(F.col(\"Quantity Ordered\")).alias(\"TotalQty\"), \\\n",
    "        F.sum(F.col(\"Sales\")) \\\n",
    "    ).orderBy(F.col(\"TotalQty\"),ascending=False).show(3)\n",
    "\n",
    "# Listar las 10 ciudades con mayores ventas (en ingresos).\n",
    "dfventas.groupBy(F.col(\"city\")).agg(F.sum(F.col(\"Sales\")).alias(\"CitySales\")).orderBy(F.desc(F.col(\"CitySales\"))).show(10)\n",
    "\n",
    "# Tabla de n칰mero de pedidos e importe por horas (campo hour extra칤do de Order Date).\n",
    "dfventas.groupBy(F.hour(F.col(\"timestamp\")).alias(\"Hour\")) \\\n",
    "    .agg(F.sum(F.col(\"Quantity Ordered\")).alias(\"TotalQty\"),F.sum(F.col(\"Sales\"))) \\\n",
    "    .orderBy(F.col(\"Hour\")).show(24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfdcd4ec-5476-46cc-91f4-c3c1cb3a2b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opciones clave por fuente\n",
    "\n",
    "# Archivos\n",
    "\n",
    "df = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"path\", \"/ruta/al/directorio\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\  # N췈 de archivos le칤dos por trigger\n",
    "    .option(\"latestFirst\", True) \\      # Leer los archivos m치s recientes primero\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba6869b-5dfc-4d3c-93c6-ef9d2185a362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Socket: s칩lo pruebas, sin tolerancia a fallos\n",
    "\n",
    "df = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ca9dbb-11e2-4270-9cb3-bcb6cf5221f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kafka\n",
    "\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\  # \"earliest\" para leer desde el inicio\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b9e94d-ea63-401a-83bf-50bb567f03aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rate: genera datos autom치ticamente, con un timestamp y value creciente\n",
    "\n",
    "df = spark.readStream.format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d5c488-92b9-46aa-bb40-9263b09bcb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tablas: 칔til para persistencia y consultas en tiempo real.\n",
    "\n",
    "df = spark.readStream.table(\"nombre_tabla\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6085913a-517f-4463-b3ee-acb132b65c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modos de salida (outputMode)\n",
    "\n",
    "Cuando escribimos un DataFrame en streaming (writeStream), debemos especificar c칩mo se actualizan los datos en el destino:\n",
    "\n",
    "| Modo      | Descripci칩n                                                                 |\n",
    "|-----------|-----------------------------------------------------------------------------|\n",
    "| append    | Solo agrega nuevas filas al destino. No admite actualizaciones o eliminaciones. |\n",
    "| complete  | Escribe todos los resultados en cada trigger. 칔til para agregaciones.       |\n",
    "| update    | Similar a append, pero actualiza filas ya procesadas. Necesario cuando se usan operaciones de estado (como agregaciones incrementales). |\n",
    "\n",
    "## Triggers (trigger)\n",
    "\n",
    "Define cada cu치nto tiempo se procesan los datos:\n",
    "\n",
    "| Trigger                              | Descripci칩n                                                      |\n",
    "|--------------------------------------|------------------------------------------------------------------|\n",
    "| trigger(processingTime=\"10 seconds\") | Ejecuta la consulta cada 10 segundos.                           |\n",
    "| trigger(once=True)                  | Ejecuta la consulta solo una vez, procesando todos los datos disponibles. |\n",
    "| trigger(continuous=\"1 second\")      | Para baja latencia (solo con fuentes compatibles como Kafka).   |\n",
    "\n",
    "## Destinos (sinks)\n",
    "\n",
    "El sink define d칩nde se escriben los datos procesados:\n",
    "\n",
    "| Sink       | Descripci칩n                                                        |\n",
    "|------------|--------------------------------------------------------------------|\n",
    "| console    | Muestra los resultados en la consola (칰til para pruebas).          |\n",
    "| memory     | Guarda en memoria (puede ser consultado con SQL).                  |\n",
    "| file       | Escribe en formato JSON, Parquet, CSV, etc.                       |\n",
    "| kafka      | Escribe mensajes en un topic de Apache Kafka.                     |\n",
    "| foreach    | Permite escribir en un destino personalizado (API de Python/Java). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ffef31-1076-4080-a72a-3765e405d5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tolerancia a fallos y Checkpoints\n",
    "Spark Streaming garantiza tolerancia a fallos con Checkpoints y WAL (Write-Ahead Logs).\n",
    "\n",
    "Checkpointing: Almacena el estado del streaming en un directorio para recuperarlo en caso de fallo.\n",
    "Habilitaci칩n de checkpoints:\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/ruta/salida\") \\\n",
    "    .option(\"checkpointLocation\", \"/ruta/checkpoints\") \\\n",
    "    .start()\n",
    "Esto permite recuperaci칩n autom치tica en caso de fallo del nodo o reinicio de la aplicaci칩n.\n",
    "\n",
    "游늱 Windowing (Ventanas temporales)\n",
    "Permite agrupar datos en intervalos de tiempo en lugar de procesar cada evento individualmente.\n",
    "\n",
    "Ejemplo: Contar eventos en ventanas de 10 segundos con 5 segundos de solapamiento:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "df_window = df.groupBy(window(\"timestamp\", \"10 seconds\", \"5 seconds\")).count()\n",
    "游댳 칔til en casos como:\n",
    "\n",
    "Contar palabras en un flujo de datos por minuto.\n",
    "Agregaciones en tiempo real con datos que llegan en diferentes momentos.\n",
    "游눦 Watermarking\n",
    "Controla la cantidad de datos antiguos que se retienen en memoria, permitiendo manejar retrasos en la llegada de datos sin consumir demasiados recursos.\n",
    "\n",
    "Ejemplo: Mantener datos de hasta 10 minutos en un groupBy basado en ventana de tiempo:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "df_watermarked = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count()\n",
    "游댳 Evita acumulaci칩n de estado indefinida y mejora la escalabilidad del sistema.\n",
    "\n",
    "游댕 Joins en Streaming\n",
    "Se pueden realizar uniones entre:\n",
    "\n",
    "Streaming vs Batch 九\n",
    "Streaming vs Streaming (con restricciones) 游뚾\n",
    "Ejemplo de join entre un flujo de datos y una tabla est치tica:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "df_streaming.join(df_estatico, \"id\", \"inner\")\n",
    "Para unir dos streams, es necesario que al menos uno tenga Watermark:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "df1.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .join(df2.withWatermark(\"timestamp\", \"10 minutes\"), \"id\", \"inner\")\n",
    "游댳 Uso com칰n en correlaci칩n de eventos, como unir registros de compras en streaming con datos de clientes en batch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3432906384973617,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
