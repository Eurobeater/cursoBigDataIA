{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372cad7e-189f-49b7-a879-3f4e74086036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Introducción a DataFrames:\n",
    "## Conceptos básicos:\n",
    "Los DataFrames son una abstracción de datos estructurados, organizados en filas y columnas, con un esquema definido. Esta estructura facilita la manipulación y el análisis de datos utilizando las APIs de Spark SQL.\n",
    "\n",
    "## Creación de DataFrames:\n",
    "- **Desde RDDs**: Los DataFrames pueden crearse a partir de RDDs (colecciones distribuidas de datos). La creación de un DataFrame desde un RDD permite trabajar con datos no estructurados transformándolos en un formato tabular.\n",
    "- **Desde archivos**: Spark SQL permite la creación de DataFrames desde varios formatos de archivos, como CSV, JSON y Parquet. Puedes cargar estos archivos directamente en un DataFrame utilizando la API de Spark SQL. También se pueden usar otros formatos de archivo.\n",
    "- **Desde tablas Hive**: Puedes crear DataFrames a partir de tablas existentes en Hive, aprovechando el metastore de Hive.\n",
    "- **Otras fuentes**: Spark puede leer datos de diversas fuentes incluyendo bases de datos relacionales mediante JDBC, NoSQL, ORC, y otros sistemas de almacenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cfef7f1-5433-434d-abb5-7b23fca672bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Cargar datos desde un RDD:\n",
    "Para convertir un RDD en un DataFrame, se utiliza la función `toDF()` o `createDataFrame()`.\n",
    "\n",
    "- **toDF()**: Infiere el esquema del DataFrame a partir del RDD, normalmente usado con una tupla o lista en Python.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toDF.html (similar, la original no está documentada en https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)\n",
    "- **createDataFrame()**: Permite especificar explícitamente el esquema (`StructType`) del DataFrame.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a36ba3b-95fd-4ecc-8532-aefc34e6d84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos sesión\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDDtoDF\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b37dca2-339c-4a46-80f8-39e65639626c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version: \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c014c55-b5a0-48ee-9aca-55e5aded6fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(\"Spark version: \", sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7eb31e-0e22-4e9c-8dd2-5021b2e74623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Ejemplo con toDF():\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 34), (\"Bob\", 23)])\n",
    "df = rdd.toDF([\"name\", \"age\"])\n",
    "df.show()\n",
    "\n",
    "## Ejemplo sobre rdd cargado de un fichero (error porque no consigue inferir el esquema)\n",
    "lines = sc.textFile(\"dbfs:/FileStore/u.data\")\n",
    "df_peliculas = lines.toDF()\n",
    "df_peliculas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1269e2e-f012-427c-a51e-f767bcf9160f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Ejemplo con createDataFrame() especificando el esquema:\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "esquema = StructType([\n",
    "            StructField(\"usuario\",IntegerType(),False),\n",
    "            StructField(\"pelicula\",IntegerType(),False),\n",
    "            StructField(\"rating\", IntegerType(),False),\n",
    "            StructField(\"timestamp\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "lineas_rdd = sc.textFile(\"dbfs:/FileStore/u.data\")\n",
    "\n",
    "# Convertimos un rdd de filas en un rdd de listas de 4 strings\n",
    "lineas_rdd_cadenas = lineas_rdd.map(lambda x: x.split())\n",
    "\n",
    "# Como el esquema espera enteros, convertimos el rdd a listas de 4 enteros\n",
    "lineas_rdd_enteros = lineas_rdd_cadenas.map(lambda x: [int(x[0]), int(x[1]), int(x[2]), int(x[3])])\n",
    "\n",
    "df_peliculas = spark.createDataFrame(lineas_rdd_enteros,esquema)\n",
    "df_peliculas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e29b00-9396-47ff-9a38-897a805d9d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mismo caso pero definiendo la función para el map\n",
    "def leerFila(fila):\n",
    "    lista = fila.split()\n",
    "    return [int(lista[0]),int(lista[1]),int(lista[2]),int(lista[3])]\n",
    "\n",
    "lineas_rdd_filas = sc.textFile(\"dbfs:/FileStore/u.data\")\n",
    "lineas_rdd_enteros2 = lineas_rdd_filas.map(leerFila)\n",
    "\n",
    "df_peliculas2 = spark.createDataFrame(lineas_rdd_enteros2,esquema)\n",
    "df_peliculas2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9754af4c-f9db-42dd-9101-777b80d316aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Cargar datos desde ficheros CSV:\n",
    "\n",
    "#### Sintaxis:\n",
    "Se utiliza `spark.read.csv()`. Se pueden especificar opciones como `header` para indicar si el archivo tiene encabezado e `inferSchema` para que Spark infiera los tipos de datos.\n",
    "\n",
    "- `header` indica si la primera línea del archivo CSV contiene los nombres de las columnas.\n",
    "- `inferSchema` permite a Spark determinar automáticamente los tipos de datos de cada columna.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "\n",
    "#### Consideraciones\n",
    "- **Rutas de archivos**: Asegúrate de proporcionar las rutas correctas a tus archivos CSV y Parquet.\n",
    "- **Esquema**: Si no se utiliza `inferSchema` al leer archivos CSV, el esquema del DataFrame debe especificarse explícitamente.\n",
    "- **DataFrames**: Los DataFrames proporcionan una forma de procesar y analizar datos estructurados. A diferencia de los RDDs, los DataFrames están basados en un esquema, es decir, conocen los nombres y tipos de las columnas de un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66430627-0af8-4efd-bd39-a93d54b45e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Desde un fichero CSV\n",
    "df=spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/olive.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "492a0046-27f6-449e-9636-0fc0e73eb0d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Cargar datos desde ficheros Parquet:\n",
    "Se utiliza `spark.read.parquet()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html  \n",
    "\n",
    "#### Consideraciones\n",
    "- **Esquema**: El esquema se almacena en el mismo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45369da-016d-4cc3-85d3-05e774ca8a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Desde un único fichero parquet\n",
    "df=spark.read.parquet(\"dbfs:/FileStore/palm.parquet\")\n",
    "df.show(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1649034b-2028-44be-ae29-0f525bc1db54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. API de DataFrames:\n",
    "Para realizar transformaciones en un DataFrame en Spark con Python, se utilizan diversas funciones que permiten modificar, seleccionar, o agregar datos. Esta es la sintaxis y ejemplos de uso de algunas de las transformaciones más comunes:\n",
    "\n",
    "\n",
    "## Transformaciones:\n",
    "- **select**: Permite seleccionar columnas específicas de un DataFrame.\n",
    "- **filter**: Permite filtrar filas basadas en una condición.\n",
    "- **withColumn**: Permite añadir nuevas columnas o modificar las existentes.\n",
    "- **Otras transformaciones**: La API incluye otras transformaciones para manipular los datos como groupBy, sort y join. También permite crear funciones definidas por el usuario para manipulación personalizada de datos.\n",
    "\n",
    "## Acciones:\n",
    "- **show**: Muestra las primeras filas de un DataFrame.\n",
    "- **count**: Cuenta el número de filas en un DataFrame.\n",
    "- **collect**: Retorna todos los elementos de un DataFrame al driver (cuidado con el uso en grandes datasets).\n",
    "- **Otras acciones**: Incluyen take, takeSample y describe para obtener información y estadísticas sobre los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3089b131-377f-47c9-b697-e59bd96b4d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consideraciones:\n",
    "\n",
    "- **Inmutabilidad**: Los DataFrames son inmutables; cada transformación crea un nuevo DataFrame.\n",
    "- **show()**: La función `show()` se utiliza para mostrar una muestra de los datos resultantes tras una transformación.\n",
    "- **Importaciones**: Algunas funciones requieren importaciones adicionales desde `pyspark.sql.functions`, como `col`, `lit`, `expr`, `avg`, `count`, etc.\n",
    "- **Expresiones SQL**: Puedes usar expresiones SQL con `expr()` y `selectExpr()` para transformaciones más complejas.\n",
    "- **Columnas**: Las columnas se pueden referenciar usando su nombre como string, usando la notación de corchetes sobre el DataFrame o con la función `col()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf8a0a8-53d1-4ffc-a284-369c836b69a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. select():\n",
    "Se utiliza para seleccionar un subconjunto de columnas de un DataFrame. También se puede usar `selectExpr()` para seleccionar columnas con expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50c8ffd-cf42-491d-92ab-e51a8c586628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/olive.csv\")\n",
    "\n",
    "df.select(\"Country\", \"Year\", \"Production\").show() \n",
    "df.select('*').show()\n",
    "df.select(df.Country, (df.Production * 1000).alias('Production (Tm)')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c397fbf1-0f69-4641-b8f7-c8ba492d17ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. filter() o where():\n",
    "Se utiliza para filtrar filas basadas en una condición. `filter()` y `where()` son sinónimos.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa921fc8-ccfb-4256-ac3c-4592532c37ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((df[\"Country\"] == \"Spain\") & (df[\"Year\"] < 1984)).select(\"Country\", \"Year\", \"Production\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b9e469-34c2-4e8c-87d7-bcbf4edc8d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. withColumn():\n",
    "Se utiliza para añadir una nueva columna o reemplazar una existente. La función `lit()` crea una columna con un valor literal y `expr()` permite usar expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45bea72-ca67-4892-82a5-3a3126710b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "from datetime import datetime\n",
    "\n",
    "df=spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/olive.csv\")\n",
    "df_Spain = df.filter((df[\"Country\"] == \"Spain\") | (df[\"Country\"] == \"European Union\")).select(\"Country\", \"Year\", \"Production\")\n",
    "\n",
    "df_Spain = df_Spain.withColumn(\"Production (Tm)\", df_Spain[\"Production\"]*1000)\n",
    "df_Spain = df_Spain.withColumn(\"Region\", expr(\"CASE WHEN Year <= 1990 THEN 'Spain' ELSE 'EU' END\"))\n",
    "\n",
    "current_year = datetime.now().year\n",
    "df_Spain = df_Spain.withColumn(\"Diff_Years\", lit(current_year) - df_Spain[\"Year\"])\n",
    "\n",
    "df_Spain.show(df_Spain.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc116d9-6f28-4c6c-b72f-0838571aec98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. withColumnRenamed():\n",
    "Se utiliza para renombrar una columna existente.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefdc6c9-33ad-4a19-ad76-93d161732906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Spain.withColumnRenamed(\"Country\", \"País\").withColumnRenamed(\"Year\", \"Año\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a6a02e-25f3-453f-99de-c18ec2c9a9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. groupBy():\n",
    "Se utiliza para agrupar filas con valores iguales en una columna y realizar operaciones de agregación. Se combina con funciones de agregación como `count()`, `sum()`, `avg()`, `min()`, `max()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e7a9e5-73c7-45f9-a489-f4bdcb17ce06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podemos realizarlo de dos formas: utilizando funciones de F o con diccionarios. \n",
    "# La primera es más clara y permite realizar varias agregaciones sobre la misma columna.\n",
    "\n",
    "df=spark.read.parquet(\"dbfs:/FileStore/palm.parquet\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "df.select(df.Country, df.Year, df.Production) \\\n",
    "    .groupBy(\"Country\") \\\n",
    "    .agg(\n",
    "        F.sum(\"Production\").alias(\"TotalProd\"),\n",
    "        F.max(\"Production\").alias(\"MaxProd\")\n",
    "    ) \\\n",
    "    .orderBy(\"TotalProd\",ascending=False) \\\n",
    ".show(4)\n",
    "\n",
    "df.select(df.Country, df.Year, df.Production) \\\n",
    "    .groupBy(\"Country\") \\\n",
    "    .agg({\"Production\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(Production)\", \"TotalProd\") \\\n",
    "    .orderBy(\"TotalProd\", ascending=False) \\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd8f953-f417-4fde-9ec9-695f8ba31716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. sort() o orderBy():\n",
    "Se utiliza para ordenar las filas del DataFrame. `sort()` y `orderBy()` son equivalentes y pueden usar el orden ascendente (`asc`) o descendente (`desc`).  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6adf5f6-c556-411f-bfe4-c17ba44c6396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df=spark.read.parquet(\"dbfs:/FileStore/palm.parquet\")\n",
    "df = df.select(df.Country, df.Year, df.Production).groupBy(\"Country\",\"Year\").agg(F.sum(\"Production\").alias(\"TotalProd\"))\n",
    "df.sort(\"Country\").show(5)\n",
    "df.sort(df[\"Country\"].desc()).show(5)\n",
    "df.orderBy(\"TotalProd\").show(5)\n",
    "df.orderBy(df[\"TotalProd\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934b6fef-2f7c-4489-8369-b48bd3d04877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. drop():\n",
    "Se utiliza para eliminar una o varias columnas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9a8470-9feb-4931-99f5-3a5dec3d8afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Spain.show(5)\n",
    "df_Spain_reducido = df_Spain.drop(\"Year\", \"Diff_Years\")\n",
    "df_Spain_reducido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39039dd-a46a-4c37-9956-cc5147f4111a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. distinct():\n",
    "Se utiliza para eliminar las filas duplicadas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428e6a3b-d56e-4895-90d3-d093d28ae8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Spain_reducido.distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc887e0-1adf-4f97-a545-050d13b695e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. join():\n",
    "Se utiliza para combinar dos DataFrames basados en una o varias columnas en común. Se puede especificar el tipo de join: 'inner', 'outer', 'left_outer', 'right_outer', o 'leftsemi'.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfce3bc6-614f-4146-9dff-1073171a9075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Definir el esquema del DataFrame de ratings\n",
    "esquemaRatings = StructType([\n",
    "    StructField(\"UserID\", IntegerType(), True),\n",
    "    StructField(\"MovieID\", IntegerType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Cargar el archivo de ratings u-data en un DataFrame, con separador el tabulador \\t\n",
    "dfRatings = spark.read.csv(\"dbfs:/FileStore/u.data\", sep=\"\\t\", schema=esquemaRatings, header=False)\n",
    "\n",
    "# Definir esquema para el DataFrame de películas\n",
    "esquemaPeliculas = StructType([\n",
    "    StructField(\"MovieID\", IntegerType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"ReleaseDate\", StringType(), True),\n",
    "    StructField(\"EmptyColumn\", StringType(), True),\n",
    "    StructField(\"IMDB_URL\", StringType(), True),\n",
    "    StructField(\"Unknown\", IntegerType(), True),\n",
    "    StructField(\"Action\", IntegerType(), True),\n",
    "    StructField(\"Adventure\", IntegerType(), True),\n",
    "    StructField(\"Animation\", IntegerType(), True),\n",
    "    StructField(\"Children\", IntegerType(), True),\n",
    "    StructField(\"Comedy\", IntegerType(), True),\n",
    "    StructField(\"Crime\", IntegerType(), True),\n",
    "    StructField(\"Documentary\", IntegerType(), True),\n",
    "    StructField(\"Drama\", IntegerType(), True),\n",
    "    StructField(\"Fantasy\", IntegerType(), True),\n",
    "    StructField(\"FilmNoir\", IntegerType(), True),\n",
    "    StructField(\"Horror\", IntegerType(), True),\n",
    "    StructField(\"Musical\", IntegerType(), True),\n",
    "    StructField(\"Mystery\", IntegerType(), True),\n",
    "    StructField(\"Romance\", IntegerType(), True),\n",
    "    StructField(\"SciFi\", IntegerType(), True),\n",
    "    StructField(\"Thriller\", IntegerType(), True),\n",
    "    StructField(\"War\", IntegerType(), True),\n",
    "    StructField(\"Western\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Cargar el archivo de películas en un DataFrame, con separador |\n",
    "dfPeliculas = spark.read.csv(\"dbfs:/FileStore/u.item\", sep=\"|\", schema=esquemaPeliculas, header=False)\n",
    "\n",
    "# Mostrar las 10 películas con más votos\n",
    "dfRatingsNombres = dfRatings.join(dfPeliculas,on=\"MovieID\",how=\"inner\")\n",
    "dfRatingsAgrupados = dfRatingsNombres.groupBy(\"Title\").agg(F.count(\"Title\").alias(\"Ratings\")).orderBy(\"Ratings\",ascending=False)\n",
    "dfRatingsAgrupados.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85eb1d81-32cd-4e8b-863f-887dfab28601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 10. union() o unionAll():\n",
    "Se utiliza para combinar dos DataFrames con las mismas columnas. `union()` elimina duplicados, `unionAll()` no.  \n",
    "    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html\n",
    "y https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionAll.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77ca6cf-1695-4e86-9e84-3e64186dd5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear dos dataframes con los mismos campos (películas más veces puntuadas y películas mejor puntuadas)\n",
    "# Hacer la unión y mostrarlos.\n",
    "\n",
    "# Películas más veces puntuadas\n",
    "# dfRatingsBest (Title, Ratings, MeanRating)\n",
    "\n",
    "# Películas mejor puntuadas, con más de 100 votos\n",
    "# dfRatingsTop (Title, Ratings, MeanRating)\n",
    "\n",
    "# dfRatingsBest.union(dfRatingsTop).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7d3f8d0-9f96-4f71-9077-58340a4c772b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 11. map():\n",
    "Se utiliza para aplicar una función a cada fila del DataFrame, convirtiéndolo a RDD.  \n",
    "La función map() se aplica a RDDs (Resilient Distributed Datasets), no directamente a DataFrames. Para usar map en un DataFrame, primero debes convertirlo a un RDD usando df.rdd.   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717e90f1-a94f-41c8-9315-209b12bc4fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener una lista de nombres de películas y un diccionario con el número de votos en cada puntuación:\n",
    "\n",
    "# Nos quedamos con las columnas MovieID y Rating\n",
    "dfRatings = dfRatings.select(\"MovieID\", \"Rating\")\n",
    "\n",
    "# Convertir el DataFrame de ratings a un RDD de filas\n",
    "rddFilas = dfRatings.rdd\n",
    "\n",
    "# Convertir el RDD de filas a un RDD de tuplas\n",
    "rddTuplas = rddFilas.map(lambda fila: (fila[0], (fila[1],1)))\n",
    "\n",
    "# Función para crear un diccionario con el número de votos para cada puntuación\n",
    "def crearRatingDict(tuplas):\n",
    "    RatingDict = {}\n",
    "    for rating, cont in tuplas:\n",
    "        if rating in RatingDict:\n",
    "            RatingDict[rating] += cont\n",
    "        else:\n",
    "            RatingDict[rating] = cont\n",
    "    return RatingDict\n",
    "\n",
    "# Agrupar por MovieID y agregar las puntuaciones\n",
    "rddRatingsAgrupados = rddTuplas.groupByKey().mapValues(crearRatingDict)\n",
    "\n",
    "# Volver a convertir a dataframe y hacer join con películas para obtener el nombre\n",
    "\n",
    "# Mostrar 10 películas con su nombre y puntuaciones\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ejercicios Dataframes pyspark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
