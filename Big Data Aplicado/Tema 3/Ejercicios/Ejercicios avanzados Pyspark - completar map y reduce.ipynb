{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefdde03-0857-4239-b384-7fa3c5083c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Películas más votadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030bf8f7-abe9-4626-b8e2-de2f498794f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sin nombre, solo tabla Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de075843-6c9d-414f-b5f7-0b2c7ab6d652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|movieID|count|\n+-------+-----+\n|     50|  583|\n|    258|  509|\n|    100|  508|\n|    181|  507|\n|    294|  485|\n|    286|  481|\n|    288|  478|\n|      1|  452|\n|    300|  431|\n|    121|  429|\n+-------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "# Definir el esquema del DataFrame de ratings\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Cargar el archivo de ratings u-data en un DataFrame, con separador el tabulador \\t\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"dbfs:/FileStore/u.data\")\n",
    "\n",
    "# Funciones tipo SQL para agrupar y ordenar\n",
    "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Top 10\n",
    "topMovieIDs.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866be386-5ec8-4a26-9677-a69fbe6c8bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Con nombre, haciendo inner join con películas (u.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f9259d-af09-4309-a8f0-4587bbbdcb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n|               Title|Ratings|\n+--------------------+-------+\n|    Star Wars (1977)|    583|\n|      Contact (1997)|    509|\n|        Fargo (1996)|    508|\n|Return of the Jed...|    507|\n|    Liar Liar (1997)|    485|\n|English Patient, ...|    481|\n|       Scream (1996)|    478|\n|    Toy Story (1995)|    452|\n|Air Force One (1997)|    431|\n|Independence Day ...|    429|\n+--------------------+-------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Definir el esquema del DataFrame de ratings\n",
    "esquemaRatings = StructType([\n",
    "    StructField(\"UserID\", IntegerType(), True),\n",
    "    StructField(\"MovieID\", IntegerType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Cargar el archivo de ratings u-data en un DataFrame, con separador el tabulador \\t\n",
    "dfRatings = spark.read.csv(\"dbfs:/FileStore/u.data\", sep=\"\\t\", schema=esquemaRatings, header=False)\n",
    "\n",
    "# Definir esquema para el DataFrame de películas\n",
    "esquemaPeliculas = StructType([\n",
    "    StructField(\"MovieID\", IntegerType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"ReleaseDate\", StringType(), True),\n",
    "    StructField(\"EmptyColumn\", StringType(), True),\n",
    "    StructField(\"IMDB_URL\", StringType(), True),\n",
    "    StructField(\"Unknown\", IntegerType(), True),\n",
    "    StructField(\"Action\", IntegerType(), True),\n",
    "    StructField(\"Adventure\", IntegerType(), True),\n",
    "    StructField(\"Animation\", IntegerType(), True),\n",
    "    StructField(\"Children\", IntegerType(), True),\n",
    "    StructField(\"Comedy\", IntegerType(), True),\n",
    "    StructField(\"Crime\", IntegerType(), True),\n",
    "    StructField(\"Documentary\", IntegerType(), True),\n",
    "    StructField(\"Drama\", IntegerType(), True),\n",
    "    StructField(\"Fantasy\", IntegerType(), True),\n",
    "    StructField(\"FilmNoir\", IntegerType(), True),\n",
    "    StructField(\"Horror\", IntegerType(), True),\n",
    "    StructField(\"Musical\", IntegerType(), True),\n",
    "    StructField(\"Mystery\", IntegerType(), True),\n",
    "    StructField(\"Romance\", IntegerType(), True),\n",
    "    StructField(\"SciFi\", IntegerType(), True),\n",
    "    StructField(\"Thriller\", IntegerType(), True),\n",
    "    StructField(\"War\", IntegerType(), True),\n",
    "    StructField(\"Western\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Cargar el archivo de películas en un DataFrame, con separador |\n",
    "dfPeliculas = spark.read.csv(\"dbfs:/FileStore/u.item\", sep=\"|\", schema=esquemaPeliculas, header=False)\n",
    "\n",
    "# Mostrar las 10 películas con más votos\n",
    "dfRatingsNombres = dfRatings.join(dfPeliculas,on=\"MovieID\",how=\"inner\")\n",
    "\n",
    "dfRatingsAgrupados = dfRatingsNombres.groupBy(\"Title\").agg(F.count(\"Title\").alias(\"Ratings\")).orderBy(\"Ratings\",ascending=False)\n",
    "dfRatingsAgrupados.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48cf54a-3c5b-4316-953b-1add9bbbc77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Con un diccionario, UDF y broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de445b87-228c-4d7f-a4b0-b45a8b978956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------------------+\n|movieID|count|movieTitle                   |\n+-------+-----+-----------------------------+\n|50     |583  |Star Wars (1977)             |\n|258    |509  |Contact (1997)               |\n|100    |508  |Fargo (1996)                 |\n|181    |507  |Return of the Jedi (1983)    |\n|294    |485  |Liar Liar (1997)             |\n|286    |481  |English Patient, The (1996)  |\n|288    |478  |Scream (1996)                |\n|1      |452  |Toy Story (1995)             |\n|300    |431  |Air Force One (1997)         |\n|121    |429  |Independence Day (ID4) (1996)|\n+-------+-----+-----------------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "import codecs\n",
    "\n",
    "# Genera un diccionario con código y nombre de película\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    lines = sc.textFile(\"dbfs:/FileStore/u.item\")\n",
    "    collected_lines = lines.collect()  # Convierte el RDD en una lista\n",
    "    for line in collected_lines:\n",
    "        fields = line.split('|')\n",
    "        movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Carga el diccionario en una variable distribuida a todos los nodos con broadcast\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
    "\n",
    "# Crea el esquema de rating\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Carga df de ratings\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"dbfs:/FileStore/u.data\")\n",
    "\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "# Crea una función definida por usuario (UDF) para buscar nombres de películas en el diccionario distribuido a partir del código\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "lookupNameUDF = func.udf(lookupName)\n",
    "\n",
    "# Añade la columna nombre de película usando la función UDF\n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(func.col(\"movieID\")))\n",
    "\n",
    "# Ordena los resultados\n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Muestra los 10 primeros\n",
    "sortedMoviesWithNames.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df213d1-b2f6-433f-8165-4f146aa8da42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Obtener una lista de nombres de películas y un diccionario con el número de votos en cada puntuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f23e6b-16ae-4988-a617-01c28bb19c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con las columnas MovieID y Rating\n",
    "dfRatings = dfRatings.select(\"MovieID\", \"Rating\")\n",
    "\n",
    "# Convertir el DataFrame de ratings a un RDD de filas\n",
    "rddFilas = dfRatings.rdd\n",
    "\n",
    "# Convertir el RDD de filas a un RDD de tuplas\n",
    "rddTuplas = rddFilas.map(lambda fila: (fila[0], (fila[1],1)))\n",
    "\n",
    "# Función para crear un diccionario con el número de votos para cada puntuación\n",
    "def crearRatingDict(tuplas):\n",
    "    RatingDict = {}\n",
    "    for rating, cont in tuplas:\n",
    "        if rating in RatingDict:\n",
    "            RatingDict[rating] += cont\n",
    "        else:\n",
    "            RatingDict[rating] = cont\n",
    "    return RatingDict\n",
    "\n",
    "# Agrupar por MovieID y agregar las puntuaciones\n",
    "rddRatingsAgrupados = rddTuplas.groupByKey().mapValues(crearRatingDict)\n",
    "\n",
    "# Volver a convertir a dataframe y hacer join con películas para obtener el nombre\n",
    "\n",
    "# Mostrar 10 películas con su nombre y puntuaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e1202d-7d0a-4ce2-b434-03f80316e210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Superhéroe más relacionado\n",
    "En cada línea aparece un código de superhéroe y los códigos de otros superhéroes que aparecen con él en algún comic. Puede aparecer repetido en más de una línea.\n",
    "\n",
    "Ficheros: Marvel-names.txt, Marvel-graph.txt\n",
    "\n",
    "Obtener cuál es el superhéroe que más relaciones tiene con otros superhéroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eabd017-2584-4fda-9437-5d2c085ce28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA es el superhéroe más relacionado con 1933 relaciones.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"dbfs:/FileStore/Marvel_names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"dbfs:/FileStore/Marvel_graph.txt\")\n",
    "\n",
    "# Trim de la columna para eliminar posibles espacios duplicados\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \" es el superhéroe más relacionado con \" + str(mostPopular[1]) + \" relaciones.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1497bedb-8db4-45d5-96c0-ce8b7b83b806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distancia entre superhéroes\n",
    "Grado de separación entre dos superhéroes, calculándolo a partir de las apariciones conjuntas en un comic.\n",
    "\n",
    "Utilizamos algoritmo Breadth-first search: recorre un árbol o grafo nivel por nivel, comenzando desde la raíz (o nodo inicial) y explorando todos los nodos vecinos en el nivel actual antes de moverse al siguiente nivel.\n",
    "\n",
    "BFS es útil para encontrar la ruta más corta en grafos no ponderados y para explorar todos los nodos a una cierta \"profundidad\" del nodo inicial.\n",
    "\n",
    "Pasos del algoritmo:\n",
    "\n",
    "- Inicialización:\n",
    "\n",
    "    - Coloca el nodo inicial en una cola (queue).\n",
    "    - Marca el nodo inicial como visitado.\n",
    "\n",
    "- Proceso de recorrido:\n",
    "\n",
    "    - Mientras la cola no esté vacía:\n",
    "        - Saca (dequeue) el nodo al frente de la cola.\n",
    "        - Procesa el nodo (por ejemplo, imprime su valor).\n",
    "        - Para cada nodo vecino no visitado:\n",
    "            - Marca el vecino como visitado.\n",
    "            - Añade (enqueue) el vecino a la cola.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaec909e-dac0-4f24-96fc-5a1ef9359a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\nB\nC\nD\nE\nF\nG\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo en Python:\n",
    "from collections import deque\n",
    "\n",
    "def bfs(tree, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([start_node])\n",
    "    visited.add(start_node)\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        print(node)  # Procesa el nodo\n",
    "        \n",
    "        for neighbor in tree[node]:\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "\n",
    "# Ejemplo de uso\n",
    "tree = {\n",
    "    'A': ['B', 'C'],\n",
    "    'B': ['D', 'E'],\n",
    "    'C': ['F', 'G'],\n",
    "    'D': [],\n",
    "    'E': [],\n",
    "    'F': [],\n",
    "    'G': []\n",
    "}\n",
    "\n",
    "bfs(tree, 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b60083c-2abc-41d7-84e7-fb6835c52cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BFS map-reduce ##\n",
    "**Objetivo:** Encontrar el camino más corto entre un nodo de inicio y un nodo destino en un grafo.  \n",
    "**Método:** Se utiliza un algoritmo de búsqueda en anchura (BFS) basado en operaciones de map-reduce.  \n",
    "**Estrategia:**  \n",
    "  - Cada nodo se representa con (nodo, ([vecinos], 9999, Pend))   \n",
    "    - Lista de vecinos.  \n",
    "    - Distancia: inicialmente 9999.  \n",
    "    - Estado: Pend (no visitado), Doing (en cola para expandir) y Done (procesado).  \n",
    "  - El nodo inicial se representa con distancia 0 y estado Doing: (N1, ([N2, N3], 0, Doing)\n",
    "  - En cada iteración se llama a las funciones map y reduce:\n",
    "    - Map: \n",
    "      - Expande nodos Doing, generando registros para sus vecinos con la distancia incrementada: (N2, ([], 1, Doing)), (N3, ([], 1, Doing))\n",
    "      - Reproduce nodo expandido como finalizado: (N1, ([N2, N3], 0, Done))\n",
    "      - El resto de nodos se mantiene igual.\n",
    "    - Reduce:\n",
    "      - Combina registros de cada nodo:\n",
    "      - Une conjunto de nodos adyacentes.\n",
    "      - Mínimo de distancias.\n",
    "      - Estado más avanzado.\n",
    "  - Se utiliza un acumulador para detectar cuando se alcanza el Nodo destino.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a25b81-18e6-4d4e-acd3-b96bf943de91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Iteración</th>\n",
    "            <th>Datos Iniciales</th>\n",
    "            <th>Resultado Map</th>\n",
    "            <th>Resultado Reduce</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Iteración 1</td>\n",
    "            <td>- N1: ([2,3], 0, <b>Doing</b>)<br>- N2: ([4], 9999, N/A)<br>- N3: ([4,5], 9999, N/A)<br>- N4: ([], 9999, N/A)<br>- N5: ([], 9999, N/A)</td>\n",
    "            <td>Se expande nodo 1 (<b>Doing</b>):<br> - Emite para N2: (2, ([], 1, <b>Doing</b>))<br> - Emite para N3: (3, ([4,5], 1, <b>Doing</b>))<br> - Reemite N1: (1, ([2,3], 0, <b>Done</b>))</td>\n",
    "            <td>Agrupación por nodo:<br>\n",
    "                - <b>N1</b>: (1, ([2,3], 0, <b>Done</b>))<br>\n",
    "                - <b>N2</b>: Combina (2, ([], 1, <b>Doing</b>)) y (2, ([4], 9999, <b>Doing</b>)) → (2, ([4], 1, <b>Doing</b>))<br>\n",
    "                - <b>N3</b>: Combina (3, ([], 1, <b>Doing</b>)) y (3, ([4,5], 9999, <b>Doing</b>)) → (3, ([4,5], 1, <b>Doing</b>))<br>\n",
    "                - <b>N4</b>: (4, ([], 9999, N/A))<br>\n",
    "                - <b>N5</b>: (5, ([], 9999, N/A))\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Iteración 2</b></td>\n",
    "            <td>Se parte del resultado reduce de Iteración 1:<br> - N1: ([2,3], 0, <b>Done</b>)<br> - N2: ([4], 1, <b>Doing</b>)<br> - N3: ([4,5], 1, <b>Doing</b>)<br> - N4: ([], 9999, N/A)<br> - N5: ([], 9999, N/A)</td>\n",
    "            <td>Se expanden nodos 2 y 3 (<b>Doing</b>):<br>\n",
    "                - <b>N2</b> (<b>Doing</b>):<br> -- Emite para N4: (4, ([], 2, <b>Doing</b>))<br> -- Reemite N2: (2, ([4], 1, <b>Done</b>))<br>\n",
    "                - <b>N3</b> (<b>Doing</b>):<br> -- Emite para N4: (4, ([], 2, <b>Doing</b>))<br> -- Emite para N5: (5, ([], 2, <b>Doing</b>))<br> -- Reemite N3: (3, ([4,5], 1, <b>Done</b>))\n",
    "            </td>\n",
    "            <td>Agrupación por nodo:<br>\n",
    "                - <b>N1</b>: (1, ([2,3], 0, <b>Done</b>))<br>\n",
    "                - <b>N2</b>: Combina (2, ([], 2, <b>Doing</b>)) y (2, ([4], 1, <b>Done</b>)) → (2, ([4], 1, <b>Done</b>))<br>\n",
    "                - <b>N3</b>: Combina (3, ([], 2, PE)) y (3, ([4,5], 1, PR)) → (3, ([4,5], 1, <b>Done</b>))<br>\n",
    "                - <b>N4</b>: Combina (4, ([], 2, <b>Doing</b>)) [de N2] y (4, ([], 2, <b>Doing</b>)) [de N3] y (4, ([], 9999, N/A)) → (4, ([], 2, <b>Doing</b>))<br>\n",
    "                - <b>N5</b>: Combina (5, ([], 2, <b>Doing</b>)) y (5, ([], 9999, N/A)) → (5, ([], 2, <b>Doing</b>))\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5bf3f2-35f4-49ad-b412-e0b3008c89f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando iteración # 1\nProcesando 8330 valores.\nProcesando iteración # 2\nProcesando 220615 valores.\nSe ha localizado el objetivo. Ramas paralelas en las que se ha alcanzado: 1\n"
     ]
    }
   ],
   "source": [
    "startHeroID = 5306 #SpiderMan\n",
    "targetHeroID = 14  #ADAM 3,031\n",
    "\n",
    "# Acumulador para indicar que hemos encontrado el objetivo en el recorrido horizontal del árbol\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    status = 'N/A' # No alcanzado\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startHeroID):\n",
    "        status = 'Doing'  # En proceso\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, status))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(\"dbfs:/FileStore/Marvel_graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "\n",
    "def bfsMap(node):\n",
    "    heroID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    status = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if status == 'Doing':\n",
    "        for connectionHero in connections:\n",
    "            # Añade un nodo por cada conexión\n",
    "            results.append((connectionHero, ([], distance + 1, 'Doing')))\n",
    "\n",
    "\n",
    "            # Si se alcanza el nodo buscado, se incrementa el contador\n",
    "            if connectionHero == targetHeroID:\n",
    "                hitCounter.add(1)\n",
    "        \n",
    "        # Añade el nodo recibido como procesado\n",
    "        results.append((heroID, (connections, distance, 'Done')))\n",
    "\n",
    "    else:\n",
    "        # Añade el nodo recibido sin procesar\n",
    "        results.append((heroID, (connections, distance, status)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    connections1 = data1[0]\n",
    "    connections2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    status1 = data1[2]\n",
    "    status2 = data2[2]\n",
    "\n",
    "    # Unifica las dos listas de conexiones\n",
    "    connections = list(set(connections1 + connections2))\n",
    "    \n",
    "    # Se queda con la distancia menor\n",
    "    distance = min(distance1, distance2)\n",
    "    \n",
    "    # Se queda con el estado más avanzado\n",
    "    status_priority = {'N/A': 0, 'Pend': 1, 'Doing': 2, 'Done': 3}\n",
    "    status = status1 if status_priority[status1] > status_priority[status2] else status2\n",
    "\n",
    "    return (connections, distance, status)\n",
    "    \n",
    "\n",
    "#Programa principal\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "iterationRdd.collect()\n",
    "\n",
    "for iteration in range(1, 10):\n",
    "    print(\"Procesando iteración # \" + str(iteration))\n",
    "\n",
    "    # Expande nodos Doing, generando registros para sus vecinos con la distancia incrementada. El nodo expandido se añade como finalizado. \n",
    "    # El resto de nodos se queda igual.\n",
    "    # Si se alcanza el nodo buscado, se incrementa el acumulador para indicar que hemos terminado.\n",
    "    \n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Se ejecuta la acción mapped.count() para forzar la evaluación del RDD y la actualización del acumulador\n",
    "    print(\"Procesando \" + str(mapped.count()) + \" valores.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Se ha localizado el objetivo. Ramas paralelas en las que se ha alcanzado: \" + str(hitCounter.value))\n",
    "        break\n",
    "\n",
    "    # Reducer combina registros de cada id, uniendo los nodos adyacentes, y dejando el número de pasos menor y el estado más avanzado\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ejercicios avanzados Pyspark - completar map y reduce",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
