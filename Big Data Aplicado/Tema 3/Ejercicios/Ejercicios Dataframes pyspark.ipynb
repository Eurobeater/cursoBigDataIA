{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4f4a4e-8aec-40a0-9987-04da52b69f77",
   "metadata": {},
   "source": [
    "# 1. Introducción a DataFrames:\n",
    "## Conceptos básicos:\n",
    "Los DataFrames son una abstracción de datos estructurados, organizados en filas y columnas, con un esquema definido. Esta estructura facilita la manipulación y el análisis de datos utilizando las APIs de Spark SQL.\n",
    "\n",
    "## Creación de DataFrames:\n",
    "- **Desde RDDs**: Los DataFrames pueden crearse a partir de RDDs (colecciones distribuidas de datos). La creación de un DataFrame desde un RDD permite trabajar con datos no estructurados transformándolos en un formato tabular.\n",
    "- **Desde archivos**: Spark SQL permite la creación de DataFrames desde varios formatos de archivos, como CSV, JSON y Parquet. Puedes cargar estos archivos directamente en un DataFrame utilizando la API de Spark SQL. También se pueden usar otros formatos de archivo.\n",
    "- **Desde tablas Hive**: Puedes crear DataFrames a partir de tablas existentes en Hive, aprovechando el metastore de Hive.\n",
    "- **Otras fuentes**: Spark puede leer datos de diversas fuentes incluyendo bases de datos relacionales mediante JDBC, NoSQL, ORC, y otros sistemas de almacenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f6bc9-0ad3-4b1d-8540-ec524ed19fc2",
   "metadata": {},
   "source": [
    "### 1. Cargar datos desde un RDD:\n",
    "Para convertir un RDD en un DataFrame, se utiliza la función `toDF()` o `createDataFrame()`.\n",
    "\n",
    "- **toDF()**: Infiere el esquema del DataFrame a partir del RDD, normalmente usado con una tupla o lista en Python.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toDF.html (similar, la original no está documentada en https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)\n",
    "- **createDataFrame()**: Permite especificar explícitamente el esquema (`StructType`) del DataFrame.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409c1c2a-cb98-4f15-b8f3-a5ff0b02a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos sesión\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDDtoDF\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0753bca8-5dd8-474f-a072-b2944442dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version: \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2c4149-2796-4e85-9d6c-95a1852907d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(\"Spark version: \", sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97672425-7b61-4ee3-a334-9cd19c61f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ejemplo con toDF():\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 34), (\"Bob\", 23)])\n",
    "df = rdd.toDF([\"name\", \"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32551ad-c5b1-4bd2-a191-54af920ad603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ejemplo con createDataFrame() (Python):\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 34), (\"Bob\", 23)])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047bb4f-3bdb-4217-880d-c2ad30bb3195",
   "metadata": {},
   "source": [
    "### 2. Cargar datos desde ficheros CSV:\n",
    "\n",
    "#### Sintaxis:\n",
    "Se utiliza `spark.read.csv()`. Se pueden especificar opciones como `header` para indicar si el archivo tiene encabezado e `inferSchema` para que Spark infiera los tipos de datos.\n",
    "\n",
    "- `header` indica si la primera línea del archivo CSV contiene los nombres de las columnas.\n",
    "- `inferSchema` permite a Spark determinar automáticamente los tipos de datos de cada columna.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "\n",
    "#### Consideraciones\n",
    "- **Rutas de archivos**: Asegúrate de proporcionar las rutas correctas a tus archivos CSV y Parquet.\n",
    "- **Esquema**: Si no se utiliza `inferSchema` al leer archivos CSV, el esquema del DataFrame debe especificarse explícitamente.\n",
    "- **DataFrames**: Los DataFrames proporcionan una forma de procesar y analizar datos estructurados. A diferencia de los RDDs, los DataFrames están basados en un esquema, es decir, conocen los nombres y tipos de las columnas de un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0193e3-8ee9-453a-8866-237cc62faff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desde un fichero CSV\n",
    "df=spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"c:/BDASpark/olive.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a5513-65fe-4a89-910e-edb8f6171cc6",
   "metadata": {},
   "source": [
    "### 3. Cargar datos desde ficheros Parquet:\n",
    "Se utiliza `spark.read.parquet()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html  \n",
    "\n",
    "#### Consideraciones\n",
    "- **Esquema**: El esquema se almacena en el mismo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd22792-9f02-46aa-8576-2c3f2a6c1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desde un único fichero parquet\n",
    "df=spark.read.parquet(\"c:/BDASpark/palm.parquet\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58826f73-5ee5-44d6-843d-116f803e9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desde un directorio con múltiples archivos\n",
    "df = spark.read.parquet(\"c:/BDASpark/olive.parquet\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1314a-0fad-4041-8b07-dc82e5240db5",
   "metadata": {},
   "source": [
    "# 2. API de DataFrames:\n",
    "Para realizar transformaciones en un DataFrame en Spark con Python, se utilizan diversas funciones que permiten modificar, seleccionar, o agregar datos. Aquí te presento las sintaxis y ejemplos de uso de algunas de las transformaciones más comunes:\n",
    "\n",
    "\n",
    "## Transformaciones:\n",
    "- **select**: Permite seleccionar columnas específicas de un DataFrame.\n",
    "- **filter**: Permite filtrar filas basadas en una condición.\n",
    "- **withColumn**: Permite añadir nuevas columnas o modificar las existentes.\n",
    "- **Otras transformaciones**: El API incluye otras transformaciones para manipular los datos como groupBy, sort y join. También permite crear funciones definidas por el usuario para manipulación personalizada de datos.\n",
    "\n",
    "## Acciones:\n",
    "- **show**: Muestra las primeras filas de un DataFrame.\n",
    "- **count**: Cuenta el número de filas en un DataFrame.\n",
    "- **collect**: Retorna todos los elementos de un DataFrame al driver (cuidado con el uso en grandes datasets).\n",
    "- **Otras acciones**: Incluyen take, takeSample y describe para obtener información y estadísticas sobre los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aeecb0-18a6-409c-8d83-2e1aa982aa32",
   "metadata": {},
   "source": [
    "## Consideraciones:\n",
    "\n",
    "- **Inmutabilidad**: Los DataFrames son inmutables; cada transformación crea un nuevo DataFrame.\n",
    "- **show()**: La función `show()` se utiliza para mostrar una muestra de los datos resultantes tras una transformación.\n",
    "- **Importaciones**: Algunas funciones requieren importaciones adicionales desde `pyspark.sql.functions`, como `col`, `lit`, `expr`, `avg`, `count`, etc.\n",
    "- **Expresiones SQL**: Puedes usar expresiones SQL con `expr()` y `selectExpr()` para transformaciones más complejas.\n",
    "- **Columnas**: Las columnas se pueden referenciar usando su nombre como string, usando la notación de corchetes sobre el DataFrame o con la función `col()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419d588-fac5-46e8-97c0-8ad3184da4a2",
   "metadata": {},
   "source": [
    "### 1. select():\n",
    "Se utiliza para seleccionar un subconjunto de columnas de un DataFrame. También se puede usar `selectExpr()` para seleccionar columnas con expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867ca87-bd08-4f1a-9acf-c3adee5c16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "\n",
    "df.select(\"name\", \"age\").show() \n",
    "df.select('*').show()\n",
    "df.select(df.name, (df.age + 10).alias('age')).show()\n",
    "\n",
    "# Variante con expresiones SQL\n",
    "df.selectExpr(\"name\", \"age\", \"age * 2 as double_age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcecd5-5a18-4937-9e7a-38ccca6963ed",
   "metadata": {},
   "source": [
    "### 2. filter() o where():\n",
    "Se utiliza para filtrar filas basadas en una condición. `filter()` y `where()` son sinónimos.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33217ef-10d6-4301-91b6-0e1885a15e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"age\"] > 30).show()\n",
    "df.where(df[\"age\"] > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b553803-c205-4771-b81a-4914f21c41a8",
   "metadata": {},
   "source": [
    "### 3. withColumn():\n",
    "Se utiliza para añadir una nueva columna o reemplazar una existente. La función `lit()` crea una columna con un valor literal y `expr()` permite usar expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410feb3-6845-4393-abf5-bdc76e1ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "\n",
    "df.withColumn(\"age_plus_10\", df[\"age\"] + 10).show()\n",
    "df.withColumn(\"is_adult\", lit(True)).show()\n",
    "df.withColumn(\"age_times_2\", expr(\"age * 2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354442e-3660-4032-a635-4e151008c731",
   "metadata": {},
   "source": [
    "### 4. withColumnRenamed():\n",
    "Se utiliza para renombrar una columna existente.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547a0be-38d4-4734-89a9-51220fbe13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed(\"age\", \"years\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafba858-3fa8-4388-ac66-38593bd52a65",
   "metadata": {},
   "source": [
    "### 5. groupBy():\n",
    "Se utiliza para agrupar filas con valores iguales en una columna y realizar operaciones de agregación. Se combina con funciones de agregación como `count()`, `sum()`, `avg()`, `min()`, `max()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5898596a-dafd-4ad9-899f-5b60412c8710",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `occupation` cannot be resolved. Did you mean one of the following? [`age`, `name`].;\n'Aggregate ['occupation], ['occupation, count(1) AS count(1)#101L, avg(age#14) AS avg(age)#103]\n+- LogicalRDD [name#13, age#14], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moccupation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\acalv\\.conda\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\group.py:181\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m exprs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexprs should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exprs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exprs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acalv\\.conda\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\acalv\\.conda\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `occupation` cannot be resolved. Did you mean one of the following? [`age`, `name`].;\n'Aggregate ['occupation], ['occupation, count(1) AS count(1)#101L, avg(age#14) AS avg(age)#103]\n+- LogicalRDD [name#13, age#14], false\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"occupation\").agg({\"age\": \"avg\", \"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71792d31-1492-4c28-b991-e607fc5c4c2c",
   "metadata": {},
   "source": [
    "### 6. sort() o orderBy():\n",
    "Se utiliza para ordenar las filas del DataFrame. `sort()` y `orderBy()` son equivalentes y pueden usar el orden ascendente (`asc`) o descendente (`desc`).  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391908b7-e288-4e19-b729-f6af1b56e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(\"age\").show()\n",
    "df.sort(df[\"age\"].desc()).show()\n",
    "df.orderBy(\"name\").show()\n",
    "df.orderBy(df[\"name\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73ede4-29c0-488e-b5c8-dd79b4d6745b",
   "metadata": {},
   "source": [
    "### 7. drop():\n",
    "Se utiliza para eliminar una o varias columnas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae134a0-6a98-474c-b97b-555ecca138e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"occupation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0116945-5d86-4610-94fe-f4d649c48979",
   "metadata": {},
   "source": [
    "### 8. distinct():\n",
    "Se utiliza para eliminar las filas duplicadas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78f948-ce7c-469c-bb97-50810e6862b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1250554-1ce8-4cc4-bbb9-88cea2d2e4cf",
   "metadata": {},
   "source": [
    "### 9. join():\n",
    "Se utiliza para combinar dos DataFrames basados en una o varias columnas en común. Se puede especificar el tipo de join: 'inner', 'outer', 'left_outer', 'right_outer', o 'leftsemi'.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a29530-f491-4e25-ae6c-63170c01ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\").show()\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bcc1c-ad87-417e-86da-0232fbd613ac",
   "metadata": {},
   "source": [
    "### 10. union() o unionAll():\n",
    "Se utiliza para combinar dos DataFrames con las mismas columnas. `union()` elimina duplicados, `unionAll()` no.  \n",
    "    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html\n",
    "y https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionAll.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bec570-ee3c-4483-b605-0c6d474ace7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa85a2-bb3e-41cd-967b-0015226633dd",
   "metadata": {},
   "source": [
    "### 11. map():\n",
    "Se utiliza para aplicar una función a cada fila del DataFrame, convirtiéndolo a RDD.  \n",
    "La función map() se aplica a RDDs (Resilient Distributed Datasets), no directamente a DataFrames. Para usar map en un DataFrame, primero debes convertirlo a un RDD usando df.rdd.   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45722a76-f077-4b3a-be7d-614511530485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "df.rdd.map(lambda row: (row.name, row.age * 2)).toDF([\"name\", \"double_age\"]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
