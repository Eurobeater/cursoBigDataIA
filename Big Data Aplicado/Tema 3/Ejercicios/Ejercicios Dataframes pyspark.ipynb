{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b86dbbe-3ca9-4df9-9483-127ffc1826f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Introducción a DataFrames:\n",
    "## Conceptos básicos:\n",
    "Los DataFrames son una abstracción de datos estructurados, organizados en filas y columnas, con un esquema definido. Esta estructura facilita la manipulación y el análisis de datos utilizando las APIs de Spark SQL.\n",
    "\n",
    "## Creación de DataFrames:\n",
    "- **Desde RDDs**: Los DataFrames pueden crearse a partir de RDDs (colecciones distribuidas de datos). La creación de un DataFrame desde un RDD permite trabajar con datos no estructurados transformándolos en un formato tabular.\n",
    "- **Desde archivos**: Spark SQL permite la creación de DataFrames desde varios formatos de archivos, como CSV, JSON y Parquet. Puedes cargar estos archivos directamente en un DataFrame utilizando la API de Spark SQL. También se pueden usar otros formatos de archivo.\n",
    "- **Desde tablas Hive**: Puedes crear DataFrames a partir de tablas existentes en Hive, aprovechando el metastore de Hive.\n",
    "- **Otras fuentes**: Spark puede leer datos de diversas fuentes incluyendo bases de datos relacionales mediante JDBC, NoSQL, ORC, y otros sistemas de almacenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4271b8-06f8-44bd-bfd2-0f5e7fef86fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Cargar datos desde un RDD:\n",
    "Para convertir un RDD en un DataFrame, se utiliza la función `toDF()` o `createDataFrame()`.\n",
    "\n",
    "- **toDF()**: Infiere el esquema del DataFrame a partir del RDD, normalmente usado con una tupla o lista en Python.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toDF.html (similar, la original no está documentada en https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)\n",
    "- **createDataFrame()**: Permite especificar explícitamente el esquema (`StructType`) del DataFrame.  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dafb8b86-ba38-4b32-9c73-23ac2f163b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos sesión\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDDtoDF\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4596bd18-51f4-4551-b291-1bba16700279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.9.21 (main, Dec  4 2024, 08:53:34) \n[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version: \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706b3369-7a0c-4828-9fa0-6aa72e9404dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.3.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(\"Spark version: \", sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73165d67-0ba0-47f4-a155-1b19287c03ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 34|\n|  Bob| 23|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "## Ejemplo con toDF():\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 34), (\"Bob\", 23)])\n",
    "df = rdd.toDF([\"name\", \"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd7f6c5-50d7-4442-b12c-462aed099d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 34|\n|  Bob| 23|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "## Ejemplo con createDataFrame() (Python):\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 34), (\"Bob\", 23)])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82c5963-7d18-47d6-bcb6-971a5e5e814b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Cargar datos desde ficheros CSV:\n",
    "\n",
    "#### Sintaxis:\n",
    "Se utiliza `spark.read.csv()`. Se pueden especificar opciones como `header` para indicar si el archivo tiene encabezado e `inferSchema` para que Spark infiera los tipos de datos.\n",
    "\n",
    "- `header` indica si la primera línea del archivo CSV contiene los nombres de las columnas.\n",
    "- `inferSchema` permite a Spark determinar automáticamente los tipos de datos de cada columna.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "\n",
    "#### Consideraciones\n",
    "- **Rutas de archivos**: Asegúrate de proporcionar las rutas correctas a tus archivos CSV y Parquet.\n",
    "- **Esquema**: Si no se utiliza `inferSchema` al leer archivos CSV, el esquema del DataFrame debe especificarse explícitamente.\n",
    "- **DataFrames**: Los DataFrames proporcionan una forma de procesar y analizar datos estructurados. A diferencia de los RDDs, los DataFrames están basados en un esquema, es decir, conocen los nombres y tipos de las columnas de un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a2b392-9a60-4abf-b7aa-1043ffb32a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+\n|Country|Year|Beginning Stocks|Domestic Consumption|Ending Stocks|Exports|Feed Waste Dom. Cons.|Food Use Dom. Cons.|Imports|Industrial Dom. Cons.|Production|Total Distribution|Total Supply|\n+-------+----+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+\n|Algeria|1964|               0|                  15|            0|      3|                    0|                 15|      0|                    0|        18|                18|          18|\n|Algeria|1965|               0|                  12|            0|      5|                    0|                 12|      0|                    0|        17|                17|          17|\n|Algeria|1966|               0|                  16|            0|      0|                    0|                 16|      0|                    0|        16|                16|          16|\n|Algeria|1967|               0|                  15|            0|      7|                    0|                 15|      0|                    0|        22|                22|          22|\n|Algeria|1968|               0|                  11|            0|      7|                    0|                 11|      0|                    0|        18|                18|          18|\n|Algeria|1969|               0|                  19|            0|      3|                    0|                 19|      0|                    0|        22|                22|          22|\n|Algeria|1970|               0|                  12|            0|      1|                    0|                 12|      0|                    0|        13|                13|          13|\n|Algeria|1971|               0|                  20|            2|      1|                    0|                 20|      0|                    0|        23|                23|          23|\n|Algeria|1972|               2|                  13|            2|      2|                    0|                 13|      0|                    0|        15|                17|          17|\n|Algeria|1973|               2|                  11|            3|      4|                    0|                 11|      0|                    0|        16|                18|          18|\n|Algeria|1974|               3|                  10|            1|      0|                    0|                 10|      0|                    0|         8|                11|          11|\n|Algeria|1975|               1|                  16|            3|      0|                    0|                 16|      0|                    0|        18|                19|          19|\n|Algeria|1976|               3|                  15|            3|      0|                    0|                 15|      0|                    0|        15|                18|          18|\n|Algeria|1977|               3|                   8|            0|      0|                    0|                  8|      0|                    0|         5|                 8|           8|\n|Algeria|1978|               0|                  13|            1|      0|                    0|                 13|      0|                    0|        14|                14|          14|\n|Algeria|1979|               1|                  11|            0|      0|                    0|                 11|      0|                    0|        10|                11|          11|\n|Algeria|1980|               0|                  18|            0|      0|                    0|                 18|      0|                    0|        18|                18|          18|\n|Algeria|1981|               0|                  15|            0|      0|                    0|                 15|      0|                    0|        15|                15|          15|\n|Algeria|1982|               0|                  16|            0|      0|                    0|                 16|      0|                    0|        16|                16|          16|\n|Algeria|1983|               0|                  13|            0|      0|                    0|                 13|      0|                    0|        13|                13|          13|\n+-------+----+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Desde un fichero CSV\n",
    "#df=spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"c:/BDASpark/olive.csv\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/olive.csv\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f446300-3a54-40a2-9140-b4ba848d57c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Cargar datos desde ficheros Parquet:\n",
    "Se utiliza `spark.read.parquet()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html  \n",
    "\n",
    "#### Consideraciones\n",
    "- **Esquema**: El esquema se almacena en el mismo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a9e2cb-50e9-4cc0-a5ea-24f0e5b537f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\n|    Country|Year|Area Harvested|Beginning Stocks|Domestic Consumption|Ending Stocks|Exports|Feed Waste Dom. Cons.|Food Use Dom. Cons.|Imports|Industrial Dom. Cons.|Production|Total Distribution|Total Supply|Yield|\n+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\n|Afghanistan|1964|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1965|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1966|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1967|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1968|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1969|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1970|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1971|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1972|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1973|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1974|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1975|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1976|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1977|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1978|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1979|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1980|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1981|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1982|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1983|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Desde un único fichero parquet\n",
    "# df=spark.read.parquet(\"c:/BDASpark/palm.parquet\")\n",
    "df=spark.read.parquet(\"dbfs:/FileStore/palm.parquet\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e513038d-bbce-44bf-b0d7-38b9002bea97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\n|    Country|Year|Area Harvested|Beginning Stocks|Domestic Consumption|Ending Stocks|Exports|Feed Waste Dom. Cons.|Food Use Dom. Cons.|Imports|Industrial Dom. Cons.|Production|Total Distribution|Total Supply|Yield|\n+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\n|Afghanistan|1964|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1965|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1966|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1967|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1968|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1969|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1970|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1971|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1972|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1973|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1974|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1975|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1976|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1977|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1978|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1979|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1980|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1981|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1982|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n|Afghanistan|1983|           0.0|             0.0|                 0.0|          0.0|    0.0|                  0.0|                0.0|    0.0|                  0.0|       0.0|               0.0|         0.0|  0.0|\n+-----------+----+--------------+----------------+--------------------+-------------+-------+---------------------+-------------------+-------+---------------------+----------+------------------+------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Desde un directorio con múltiples archivos\n",
    "# df = spark.read.parquet(\"c:/BDASpark/olive.parquet\")\n",
    "df = spark.read.parquet(\"dbfs:/FileStore/palm.parquet\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d85bbe5-66b0-4619-9b11-dc57ce62cfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. API de DataFrames:\n",
    "Para realizar transformaciones en un DataFrame en Spark con Python, se utilizan diversas funciones que permiten modificar, seleccionar, o agregar datos. Aquí te presento las sintaxis y ejemplos de uso de algunas de las transformaciones más comunes:\n",
    "\n",
    "\n",
    "## Transformaciones:\n",
    "- **select**: Permite seleccionar columnas específicas de un DataFrame.\n",
    "- **filter**: Permite filtrar filas basadas en una condición.\n",
    "- **withColumn**: Permite añadir nuevas columnas o modificar las existentes.\n",
    "- **Otras transformaciones**: El API incluye otras transformaciones para manipular los datos como groupBy, sort y join. También permite crear funciones definidas por el usuario para manipulación personalizada de datos.\n",
    "\n",
    "## Acciones:\n",
    "- **show**: Muestra las primeras filas de un DataFrame.\n",
    "- **count**: Cuenta el número de filas en un DataFrame.\n",
    "- **collect**: Retorna todos los elementos de un DataFrame al driver (cuidado con el uso en grandes datasets).\n",
    "- **Otras acciones**: Incluyen take, takeSample y describe para obtener información y estadísticas sobre los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d55ddf2-456e-41a1-b0bd-590b4aaf61fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consideraciones:\n",
    "\n",
    "- **Inmutabilidad**: Los DataFrames son inmutables; cada transformación crea un nuevo DataFrame.\n",
    "- **show()**: La función `show()` se utiliza para mostrar una muestra de los datos resultantes tras una transformación.\n",
    "- **Importaciones**: Algunas funciones requieren importaciones adicionales desde `pyspark.sql.functions`, como `col`, `lit`, `expr`, `avg`, `count`, etc.\n",
    "- **Expresiones SQL**: Puedes usar expresiones SQL con `expr()` y `selectExpr()` para transformaciones más complejas.\n",
    "- **Columnas**: Las columnas se pueden referenciar usando su nombre como string, usando la notación de corchetes sobre el DataFrame o con la función `col()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd23af7-827c-4fc5-b03e-2749d8b85d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. select():\n",
    "Se utiliza para seleccionar un subconjunto de columnas de un DataFrame. También se puede usar `selectExpr()` para seleccionar columnas con expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e908175a-1907-41ab-971d-b193800e74ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice|  2|\n|  Bob|  5|\n+-----+---+\n\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 12|\n|  Bob| 15|\n+-----+---+\n\n+-----+---+----------+\n| name|age|double_age|\n+-----+---+----------+\n|Alice|  2|         4|\n|  Bob|  5|        10|\n+-----+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "\n",
    "df.select(\"name\", \"age\").show() \n",
    "df.select('*').show()\n",
    "df.select(df.name, (df.age + 10).alias('age')).show()\n",
    "\n",
    "# Variante con expresiones SQL\n",
    "df.selectExpr(\"name\", \"age\", \"age * 2 as double_age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2731b42c-aac5-45fb-8b2a-5e8c818fb659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. filter() o where():\n",
    "Se utiliza para filtrar filas basadas en una condición. `filter()` y `where()` son sinónimos.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390a6d00-0f96-41e0-bf8c-491b08b312a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n|age|name|\n+---+----+\n+---+----+\n\n+---+----+\n|age|name|\n+---+----+\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"age\"] > 30).show()\n",
    "df.where(df[\"age\"] > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427af12f-d834-403f-a3bf-b352a7b6f5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. withColumn():\n",
    "Se utiliza para añadir una nueva columna o reemplazar una existente. La función `lit()` crea una columna con un valor literal y `expr()` permite usar expresiones SQL.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebf443c-1a87-42d0-8a3f-024ffba00164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n|age| name|age_plus_10|\n+---+-----+-----------+\n|  2|Alice|         12|\n|  5|  Bob|         15|\n+---+-----+-----------+\n\n+---+-----+--------+\n|age| name|is_adult|\n+---+-----+--------+\n|  2|Alice|    true|\n|  5|  Bob|    true|\n+---+-----+--------+\n\n+---+-----+-----------+\n|age| name|age_times_2|\n+---+-----+-----------+\n|  2|Alice|          4|\n|  5|  Bob|         10|\n+---+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "\n",
    "df.withColumn(\"age_plus_10\", df[\"age\"] + 10).show()\n",
    "df.withColumn(\"is_adult\", lit(True)).show()\n",
    "df.withColumn(\"age_times_2\", expr(\"age * 2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed825af5-851f-4611-a5da-ed8d21e178d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. withColumnRenamed():\n",
    "Se utiliza para renombrar una columna existente.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13eb57f5-0dd2-4c0a-9514-8672f3fae4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|years| name|\n+-----+-----+\n|    2|Alice|\n|    5|  Bob|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"age\", \"years\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202c7b0c-783c-4c5b-a46d-89954bb1e27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. groupBy():\n",
    "Se utiliza para agrupar filas con valores iguales en una columna y realizar operaciones de agregación. Se combina con funciones de agregación como `count()`, `sum()`, `avg()`, `min()`, `max()`.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11926704-0114-453d-b04f-a827363c6384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n| name|count(1)|avg(age)|\n+-----+--------+--------+\n|Alice|       1|     2.0|\n|  Bob|       1|     5.0|\n+-----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# df.groupBy(\"occupation\").agg({\"age\": \"avg\", \"*\": \"count\"}).show()\n",
    "df.groupBy(\"name\").agg({\"age\": \"avg\", \"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19c2030-36a8-408b-a44f-4ddff34de9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. sort() o orderBy():\n",
    "Se utiliza para ordenar las filas del DataFrame. `sort()` y `orderBy()` son equivalentes y pueden usar el orden ascendente (`asc`) o descendente (`desc`).  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376fd37f-f0de-4f5f-ac10-1c16504c2ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+\n\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n+---+-----+\n|age| name|\n+---+-----+\n|  5|  Bob|\n|  2|Alice|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"age\").show()\n",
    "df.sort(df[\"age\"].desc()).show()\n",
    "df.orderBy(\"name\").show()\n",
    "df.orderBy(df[\"name\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d378acaa-df6f-4431-9132-b86597bfa2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. drop():\n",
    "Se utiliza para eliminar una o varias columnas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8afc80-d5b3-496f-8662-c94d3a91ac2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"occupation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095543f0-96b1-4adb-bba2-fd3b1df3931d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. distinct():\n",
    "Se utiliza para eliminar las filas duplicadas del DataFrame.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fbea5a-e80f-48af-938d-59bfff886290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0d5ee4-7c87-4fab-af43-a39136717479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. join():\n",
    "Se utiliza para combinar dos DataFrames basados en una o varias columnas en común. Se puede especificar el tipo de join: 'inner', 'outer', 'left_outer', 'right_outer', o 'leftsemi'.  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a320fd4-2d4c-4cae-b785-bf6c882890f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n| id| name| id|    city|\n+---+-----+---+--------+\n|  1|Alice|  1|New York|\n|  2|  Bob|  2|  London|\n+---+-----+---+--------+\n\n+---+-------+----+--------+\n| id|   name|  id|    city|\n+---+-------+----+--------+\n|  1|  Alice|   1|New York|\n|  2|    Bob|   2|  London|\n|  3|Charlie|null|    null|\n+---+-------+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Creamos dos DataFrames de ejemplo con una columna 'id' en común\n",
    "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "data2 = [(1, \"New York\"), (2, \"London\"), (4, \"Tokyo\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema=[\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame(data2, schema=[\"id\", \"city\"])\n",
    "\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\").show()\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460d7f5a-f20c-45aa-a6d1-5bbfa7a7669b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 10. union() o unionAll():\n",
    "Se utiliza para combinar dos DataFrames con las mismas columnas. `union()` elimina duplicados, `unionAll()` no.  \n",
    "    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html\n",
    "y https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionAll.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3347534d-83c6-4b96-84a9-d18e3333001c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  2|     Bob|\n|  3| Charlie|\n|  1|New York|\n|  2|  London|\n|  4|   Tokyo|\n+---+--------+\n\n+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  2|     Bob|\n|  3| Charlie|\n|  1|New York|\n|  2|  London|\n|  4|   Tokyo|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9de1331-3cdf-42ba-8723-d3951f704813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 11. map():\n",
    "Se utiliza para aplicar una función a cada fila del DataFrame, convirtiéndolo a RDD.  \n",
    "La función map() se aplica a RDDs (Resilient Distributed Datasets), no directamente a DataFrames. Para usar map en un DataFrame, primero debes convertirlo a un RDD usando df.rdd.   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58138379-57bd-40ef-a1b4-472537f567dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| name|double_age|\n+-----+----------+\n|Alice|         4|\n|  Bob|        10|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "df.rdd.map(lambda row: (row.name, row.age * 2)).toDF([\"name\", \"double_age\"]).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ejercicios Dataframes pyspark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
