{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creando modelos basados en otros modelos\n",
    "\n",
    "Hasta ahora hemos usado librerías como TensorFlow para crear nuestros propios modelos capa por capa desde cero.\n",
    "\n",
    "Ahora vamos a realizar un proceso similar, excepto que la mayoría de las capas de nuestro modelo provendrán de [Kaggle](https://www.kaggle.com/models).\n",
    "\n",
    "De hecho, vamos a utilizar dos modelos:\n",
    "1. [ResNetV2](https://arxiv.org/abs/1603.05027) - una arquitectura de modelo de visión artificial basada en redes residuales (residual network).\n",
    "2. [EfficientNet](https://arxiv.org/abs/1905.11946) - una arquitectura de visión artificial basada en redes convolucionales.\n",
    "\n",
    "En algún momento, ambos modelos han logrado la tasa de error más baja en [ImageNet (ILSVRC-2012-CLS)](http://www.image-net.org/), el estándar de oro de los benchmarks de visión artificial.\n",
    "\n",
    "\n",
    "pasos:\n",
    "1. Ir a [https://www.kaggle.com/models](https://www.kaggle.com/models).\n",
    "2. Elegir tu dominio de problema, por ejemplo, \"Imagen\" (estamos usando imágenes de alimentos).\n",
    "\n",
    "> 🤔 **Pregunta:** *Veo muchas opciones para modelos de clasificación de imágenes, ¿cómo sé cuál es el mejor?*\n",
    "\n",
    "Puedes ver una lista de modelos de última generación en [paperswithcode.com](https://www.paperswithcode.com), un recurso para recopilar los últimos resultados de trabajos de investigación en aprendizaje profundo que tienen implementaciones de código para los hallazgos que informan.\n",
    "\n",
    "Dado que estamos trabajando con imágenes, nuestro objetivo son los [modelos que mejor se desempeñan en ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet).\n",
    "\n",
    "   * La regla general aquí es, generalmente, los nombres con números más grandes significan modelos de mejor rendimiento. Por ejemplo, EfficientNetB4 tiene mejor capacidad de predicción que EfficientNetB0, sin embargo, los modelos más pequeños predicen más rápido.\n",
    "     \n",
    "7. Selecciona EfficientNet y en las variaciones de modelo, elige b0-feature-vector\n",
    "\n",
    "> 🤔 **Pregunta:** *Pensé que estábamos haciendo clasificación de imágenes, ¿por qué elegimos vector de características y no clasificación?*\n",
    "\n",
    "Gran observación. Aquí es donde entran en juego los diferentes tipos de transferencia de aprendizaje, como es, extracción de características y ajuste fino.\n",
    "\n",
    "1. **Aprendizaje por transferencia \"tal cual\"** es cuando tomas un modelo preentrenado tal cual es y lo aplicas a tu tarea sin ningún cambio.\n",
    "\n",
    "   * Por ejemplo, muchos modelos de visión artificial están preentrenados en el conjunto de datos en el conjunto de datos ImageNet, que contiene 1000 clases diferentes de imágenes. Esto significa que pasar una sola imagen a este modelo producirá 1000 valores de probabilidad de predicción diferentes (1 para cada clase).\n",
    "\n",
    "    * Esto es útil si tienes 1000 clases de imágenes que te gustaría clasificar y todas son iguales a las clases de ImageNet, sin embargo, no es útil si quieres clasificar solo un pequeño subconjunto de clases (como 10 tipos diferentes de alimentos). Los modelos con `\"/classification\"` en su nombre proporcionan este tipo de funcionalidad.\n",
    "\n",
    "2. **Aprendizaje por transferencia de extracción de características** es cuando tomas los patrones subyacentes (también llamados pesos) que un modelo preentrenado ha aprendido y ajustas sus salidas para que se adapten mejor a tu problema.\n",
    "\n",
    "  * Por ejemplo, supongamos que el modelo preentrenado que estabas usando tenía 236 capas diferentes (EfficientNetB0 tiene 236 capas), pero la capa superior produce 1000 clases porque fue preentrenada en ImageNet. Para ajustar esto a tu propio problema, podrías eliminar la capa de activación original y reemplazarla con la tuya propia pero con el número correcto de clases de salida. La parte importante aquí es que **solo las primeras capas se vuelven entrenables, el resto permanece congelado**.\n",
    "\n",
    "    * De esta manera, todos los patrones subyacentes permanecen en el resto de las capas y puedes utilizarlos para tu propio problema. Este tipo de aprendizaje por transferencia es muy útil cuando tus datos son similares a los datos con los que el modelo fue preentrenado.\n",
    "\n",
    "3. **Aprendizaje por transferencia de ajuste fino (fine tuning)** es cuando tomas los patrones subyacentes (también llamados pesos) de un modelo preentrenado y los ajustas (afinas) a tu propio problema.\n",
    "\n",
    "    * Esto generalmente significa entrenar **algunas, muchas o todas** las capas en el modelo preentrenado. Esto es útil cuando tienes un conjunto de datos grande (por ejemplo, más de 100 imágenes por clase) donde tus datos son ligeramente diferentes a los datos con los que el modelo original fue entrenado.\n",
    "\n",
    "Un flujo de trabajo común es \"congelar\" todos los patrones aprendidos en las capas inferiores de un modelo preentrenado para que no se puedan entrenar. Y luego entrenar las 2-3 capas superiores para que el modelo preentrenado pueda ajustar sus salidas a tus datos personalizados (**extracción de características**).\n",
    "\n",
    "Después de haber entrenado las 2-3 capas superiores, puedes gradualmente \"descongelar\" más y más capas y ejecutar el proceso de entrenamiento en tus propios datos para afinar aún más el modelo preentrenado.\n",
    "\n",
    "> 🤔 **Pregunta:** *¿Por qué entrenar solo las 2-3 capas superiores en la extracción de características?*\n",
    "\n",
    "Cuanto más baja es una capa en un modelo de visión artificial, es decir, cuanto más cerca está de la capa de entrada, mayores son las características que aprende. Por ejemplo, una capa inferior en un modelo de visión por computadora para identificar imágenes de gatos o perros podría aprender el contorno de las patas, mientras que, las capas más cercanas a la salida podrían aprender la forma de los dientes. A menudo, querrás que las características más grandes (los patrones aprendidos también se llaman características) permanezcan, ya que estas son similares para ambos animales, mientras que, las diferencias permanecen en las características más detalladas.\n",
    "\n",
    "![image.png](transfer_learning.png)  \n",
    "*Los diferentes tipos de aprendizaje por transferencia. Un modelo original, un modelo de extracción de características (solo cambia la cabecera) y un modelo de afinamiento fino (cambian muchas o todas las capas del modelo original).*"
   ],
   "id": "2ceaa8a7dd60e9dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ],
   "id": "471d22ee88027c1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# USAREMOS TF_KERAS Para compatibilidad de resnet50v2\n",
    "# pip install tf_keras\n",
    "import tf_keras\n",
    "print(tf_keras.__version__)"
   ],
   "id": "a9774833c9b57bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# En primer lugar, vamos a cargar nuestro dataset\n",
    "\n",
    "# Definir la forma de la imagen y el tamaño del lote\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Directorios para los conjuntos de datos de entrenamiento y prueba\n",
    "train_dir = \"./10_food_classes_10_percent/train/\"\n",
    "test_dir = \"./10_food_classes_10_percent/test/\"\n",
    "\n",
    "# Crea el conjunto de datos de entrenamiento, utiliza los parámetros batch_size, image_size y label_mode='categorical'\n",
    "\n",
    "# Crea el conjunto de datos de prueba de la  misma forma\n"
   ],
   "id": "8665cd1228951d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#importamos las librerías necesarias\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tf_keras import layers"
   ],
   "id": "8c725264be64f68f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora obtendremos las URLs de vector de características de dos arquitecturas, [EfficientNetB0 (2019)](https://www.kaggle.com/models/google/efficientnet-v2/tensorFlow2/imagenet1k-b0-feature-vector/2) y [ResNetV250 (2016)](https://www.kaggle.com/models/google/resnet-v2/tensorFlow2/50-feature-vector/1) desde Kaggle usando los pasos anteriores.\n",
    "\n",
    "Obtenemos ambos porque vamos a compararlos para ver cuál se desempeña mejor en nuestros datos."
   ],
   "id": "2c7e06011b43d7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resnet_url = \"https://www.kaggle.com/models/google/resnet-v2/tensorFlow2/50-feature-vector/1\"\n",
    "\n",
    "efficientnet_url = \"https://www.kaggle.com/models/google/efficientnet-v2/tensorFlow2/imagenet1k-b0-feature-vector/2\""
   ],
   "id": "b95be2b15dc324e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_model(model_url, num_classes=10):\n",
    "  \"\"\"Toma una URL de TensorFlow Hub y crea un modelo secuencial de Keras con ella.\n",
    "  \n",
    "  Args:\n",
    "    model_url (str): Una URL de extracción de características de TensorFlow Hub.\n",
    "    num_classes (int): Número de neuronas de salida en la capa de salida,\n",
    "      debe ser igual al número de clases objetivo, por defecto 10.\n",
    "\n",
    "  Returns:\n",
    "    Un modelo secuencial de Keras sin compilar con model_url como capa\n",
    "    de extracción de características y capa de salida densa con num_classes salidas.\n",
    "  \"\"\"\n",
    "  model = ...\n",
    "  return model"
   ],
   "id": "7476ecc726d72511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Crea un modelo basado en Resnet y compilalo\n",
   "id": "2c6fea3ecc0470c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# muestra el resumen del modelo",
   "id": "66b6540ff1d7cfa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Entrena el modelo 5 épocas: Para el entrenamiento utiliza los parámetros\n",
    "# steps_per_epoch=len(train_dataset),\n",
    "# validation_data=test_dataset,\n",
    "# validation_steps=len(test_dataset)\n"
   ],
   "id": "9eadd2b0201e7c80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ¡Impresionante!  \n",
    "\n",
    "Parece que después de solo 5 épocas, el modelo de extracción de características **ResNetV250** logró superar con creces cualquiera de las arquitecturas que creamos, alcanzando aproximadamente un **90% de precisión en el conjunto de entrenamiento** y casi un **80% de precisión en el conjunto de prueba... ¡usando solo el 10% de las imágenes de entrenamiento!**\n",
    "\n",
    "Esto demuestra el poder del **aprendizaje por transferencia**. Y es una de las principales razones por las que, siempre que intentes modelar tus propios conjuntos de datos, deberías investigar qué modelos preentrenados ya existen.\n",
    "\n",
    "Vamos a revisar las curvas de entrenamiento de nuestro modelo utilizando nuestra función `plot_loss_curves`.\n"
   ],
   "id": "1440d5887467c593"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizar los datos de validación y entrenamiento por separado\n",
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Devuelve curvas de pérdida separadas para las métricas de entrenamiento y validación.\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Visualizar pérdida (loss)\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Pérdida (Loss)')\n",
    "  plt.xlabel('Épocas')\n",
    "  plt.legend()\n",
    "\n",
    "  # Visualizar acierto (accuracy)\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Acierto (Accuracy)')\n",
    "  plt.xlabel('Épocas')\n",
    "  plt.legend();\n"
   ],
   "id": "e8e19d6e236a68b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# imprime las curvas de entrenamiento",
   "id": "ddb65c1f338c858e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ¿Cuántos parámetros del modelo se han modificado?",
   "id": "f896df9ee6df9177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Puedes ver el poder de **TensorFlow Hub** aquí. La capa de extracción de características tiene **23,564,800 parámetros**, que son patrones previamente aprendidos por el modelo en el conjunto de datos **ImageNet**. Como configuramos `trainable=False`, estos patrones permanecen congelados (no entrenables) durante el entrenamiento.\n",
    "\n",
    "Esto significa que, durante el entrenamiento, el modelo actualiza los **20,490 parámetros** de la capa de salida para adaptarse a nuestro conjunto de datos.\n",
    "\n",
    "Bien, hemos entrenado un modelo **ResNetV250**, ahora es momento de hacer lo mismo con el modelo **EfficientNetB0**.\n"
   ],
   "id": "a1531d3434101f9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Ahora crea un modelo basado en efficient_net, compila y entrena el modelo 5 épocas\n",
   "id": "4f8aafe522ee88f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# muestra las curvas de entrenamiento",
   "id": "edd2f3a105375351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# saca el resumen del modelo",
   "id": "249494730162b950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "¡Increíble! El modelo **EfficientNetB0** iguala al modelo **ResNetV250** con 1/4 de sus parámetros... nuevamente **usando solo el 10% de los datos de entrenamiento**!\n",
    "\n",
    "Con solo unas pocas líneas de código, podemos aprovechar modelos de última generación y ajustarlos a nuestro propio caso de uso."
   ],
   "id": "394c87ed0af29a22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Para bonus\n",
    "### Puedes intentar utilizar transfer learning de ResNet50 o de EfficientNet con este dataset:\n",
    "[razas de perros](https://www.kaggle.com/code/thiennguyen15/predicting-dog-species-using-resnet50)"
   ],
   "id": "81f86d1cf8a54a61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
